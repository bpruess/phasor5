#Instructions: 
#    Place in a folder with a single .txt files generated by NACshow. 
# Do not move DEV version out of dev folder. Make copy and rename if moving out of folder.

#from scipy.io.wavfile import write
import math
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter, filtfilt
import xlsxwriter
import time
from scipy.stats import linregress
import statistics as stats
import gc
#from scipy.interpolate import UnivariateSpline

''' ============== ====================== ====================
    ==============    User parameters     ===================
    ============== ====================== ===================='''
    
fs = 20000 #Hz --> Sampling frequency of the experiment. Program cannot interpret sampling rate from the data files.

#stim_trial_numbers = [21, 87, 151, 203, 267, 331] #Enter the trial numbers that stim trains occur in (see NAC file)
#stim_trial_numbers = [115, 179, 243, 307]  #Enter the trial numbers that stim trains occur in (see NAC file)
stim_trial_numbers = [115, 179, 243, 307]  #Enter the trial numbers that stim trains occur in (see NAC file)

pre_n_trials = 9
post_n_trials = 15
#pre_n_trials = 2
#post_n_trials = 2

n_channels = 2 #The number of recording channels, choose 1 or 2.
#For 2-channel data, whether even or odd- numbered trials are processed depends on whether stim trial numbers are odd or even.

#t_analyze_window = (1.0, 3.0) #[secs] start & end time of processing each trial, as a list of two numbers. Write "all" to not trim data.
t_analyze_window = "all"

filter_type = "bandpass" #choose "highpass",  "lowpass", "bandpass" or "none"
    #highpass - no phase lag. bandpass causes phase lag.
lowcut = 300 #(Hz) #Ben and my data 300.
highcut = 5000.0 #(Hz). #cox cox gunn: 3000. Ben and my data 5000.
filter_order = 2 #set at 2.

graph_size = (400,12) # Dimensions of the output graph image file. Not sure what units these are, maybe inches????????????? *
graph_x_interval = 0.05 #[seconds] or write "auto" *
graph_y_interval = 0.01 # *
spike_label_display = "Number" #Enter "None", "Number" or "Details" *
graph_focus = [0, 0] #If u want graph of interval â‰  t_analyze_window, or set to [0, 0] to graph whole  of t_analyze_window *

#* would be able to rerun without recalculating anyhing.

''' ======   STARH:  ==================
Slope      -> *primary spike detection
Thresholds -> *primary spike detection
Amplitude  -> secondary filtering
Rise time  -> secondary filtering
Half width -> secondary filtering'''

''' =====   STARH settings:   ===== '''
Slope, Slope_dt = [-0.006], 5 # dV/ sample [mV/ sample], Width in [samples] over which to take slope measurement.
Thresholds = [-0.045] #[mV], for threshold detector. Will reject spikes for which |spike amplitude| < |threshold|.
#Thresholds = [-0.025, -0.035, -0.045]  #[mV] list of numbers. Negative = downward deflecting spikes. Positive = upward deflecting spikes.
Amplitude = -0.025 #[mV] amplitudes with magnitude smaller will be rejected.
Rise_times = (0.1, 0.5) #[milliseconds], lower and upper limit of allowable risetimes. Will reject spikes with rise times outside this range. Not implemented yet.
Half_widths = 1 #Not implemented yet.
#Amplitude,Rise_times and Half_widths: Set to 0 to disable filtering.

# ====== Other spike detection parameters ==============================

baseline_width = 10 #samples to measure baseline over.
#to adjust Extrema search width, look at find_local_extrema function.

max_spike_freq = 250 #[Hz], reject spikes that occurs 1/f time after previous spike.
min_burst_freq = 20 #[Hz], the minimum frequency of consecutive spiking events for them to be considered part of the same burst.
burst_min_n = 2 #!!!!!!!Minimum number of spikes in a burst. Doesn't do anything yet:- set at 2.

# ====== Stim detection function parameters ==========
#stim_peak = 0.1 #[mV] Positive threshold to trigger stim artefact detector.
stim_peak = 0.2
stim_blanking = 1 #[ms], width of stim artefact to be blanked, around midpoint of stim artefact
stim_slope_dt = 1 #[samples], the increment in samples over which to check stim slope
#stim_slope_thresh = 0.2 #Rising edge slope of stim artefact must be greater than this. 6/2/21 woz 1
stim_slope_thresh = 0.4

stim_refractory = 0 #[ms] Period between stim and start of EPSP.
blank_refractory = "yes" #!!!!!!"yes", "no", blanks the period between stim artefact and EPSP. Auto blanks between stim and start of EPSP (Doesn't do anything yet)
#epsp_padding = 100 #[ms], gap between EPSP analysis and stim artefact.
epsp_highcut = 500 #[Hz] cutoff freq of EPSP LPF. 500Hz was original value.
epsp_length = 26 #[ms] length of EPSP after stim refractory period.
#poly_deg = 30 #under development, EPSP + pop spike removal.

# === To do:
t_display_units = "seconds" #DON'T CHANGE Yet.
spike_deflection = "down" #enter "down" or "up". Not implemented yet.
#Primary detection method: 1: slope OR threshold; 2: slope only, 3: slope AND threshold, 4: threshold only.
primary_detection_mode = 1 #currently only #1 is implemented 

''' ========= END USER PARAMETERS ====================================================== '''

''' ============== ====================== ====================
    ==============    Start of Program    ===================
    ============== ====================== ===================='''

# IMPORT DATA ===============
start_time = time.time()
master_start_time = time.time()

#Declare dictionaries:
spike_stats_ = {}
spike_parameters_ = {}
sum_ = {}
    
#load file data into one list of floats, and generate indices of gaps between trial data
directory = "/"
dir_list = os.listdir(".")
print("Text files in this directory:")
for file_name in dir_list:
    if ('.txt' in file_name[-4:]):
        print(file_name)
        file = file_name
print("====================")
print("File to be processed:")
print(file)
print("Importing file, please wait.... ")

start_time = time.time()
f = open(file, 'r')
data = np.empty(1)
data = [x.strip().split('\t') for x in f]
f.close() 
print("time doing strip split",time.time()-start_time)
start_time = time.time()
data_0 = [item[0] for item in data] #gets rid of those annoying quotations marks that each data item has when imported.
data_float = [float(x) if x!='' else 0 for x in data_0] #if gap in data, replace with a 0.
print("time converting to floats",time.time()-start_time)
del data_0
# ============================
#print("file data beginning snippet",data_float[0:10])
gap_list = [(index) for index, element in enumerate(data) if element == ['']] #generates list of indices for starts of trials 
gap_list = ([-1] + gap_list)   
n_trials = len(gap_list)-1 
n_trials = len(gap_list) 
del data
#Trial number x starts at gap_list[x]
end_time = time.time()
time_elapsed = end_time - start_time
print("Finished initial loading of data, total time elapsed (seconds): ",time_elapsed)
'''
 ====================================================
 ======              FILTER FUNCTIONS               =======:
 ===================================================='''
def high_pass_filter(data, fc, fs, order):
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='highpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = lfilter(b, a, data)
    return filtered_data

'''def low_pass_filter(data, fc, fs, order): #has phase shift
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='lowpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = lfilter(b, a, data)
    return filtered_data'''

def low_pass_filter2(data, fc, fs, order): #no phase shift
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='lowpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = filtfilt(b, a, data)
    return filtered_data

def butter_bandpass(lowcut, highcut, fs, order=filter_order):
    nyq = 0.5 * fs
    low = lowcut / nyq
    #print("low",low)
    high = highcut / nyq
    #print("high",high)
    b, a = butter(order, [low, high], btype='band')
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=filter_order): #called if bandpass
    b, a = butter_bandpass(lowcut, highcut, fs, order=filter_order)
    y = lfilter(b, a, data)
    return y

''' ============================================================
 ====== Spike detection and signal processing functions  =======:
 =============================================================='''
def sample_to_sec(samples): #converts a sample number to absolute time in seconds
    if t_analyze_window == "all":
        secs = samples/fs
    else:
        secs = samples/fs + t_analyze_window[0]
    return secs 

def sec_to_sample(secs): #converts a sample number to absolute time in seconds
    if t_analyze_window == "all":
        samples = secs*fs
    else:
        samples = int(secs + t_analyze_window[0]) * fs
    return samples 

def weighted_std(values, weights):
    average = np.average(values, weights=weights)
    variance = np.average((values-average)**2, weights=weights)
    return (math.sqrt(variance))

def generate_starts_stops(sig,spike_threshold): #filtered_signal = array, spike_threshold = voltage value
   #spiketrain_overthresh_bool declare size beforehand.
   if spike_threshold > 0: deflection = "up"
   else: deflection = "down"
   print("Spike deflection = ", deflection, " threshold ", spike_threshold)
   start_time = time.time()
   starts = [0]
   stops = []   
   spiketrain_overthresh_bool = []
   i = 0
   siglength = len(sig)
   #print("siglength from generate starts stops ",siglength)
   spiketrain_overthresh_bool = [0] * siglength    #spiketrain_overthresh_bool declare size beforehand.
   #print("Spike detector running.....")
   for i in range(siglength):
      '''progress_indicator = divmod(i, fs)
      if progress_indicator[1] == 0:
          print(i/fs, "seconds processed")'''
          
      if deflection == "up":
          #Upward spike deflection
          #print("debug sig, spike_threshold",sig[i],",",spike_threshold)
          if float(sig[i]) >= float(spike_threshold):  # 
             #spiketrain_overthresh_bool = np.append(spiketrain_overthresh_bool, "1")
             spiketrain_overthresh_bool[i] = 1
             #print("      threshold sample")
          else:
             #spiketrain_overthresh_bool = np.append(spiketrain_overthresh_bool, "0")
             spiketrain_overthresh_bool[i] = 0
      elif deflection == "down":     
          #Downward spike deflection:
          if float(sig[i]) <= float(spike_threshold):  
             spiketrain_overthresh_bool[i] = 1
             #print("  threshold sample, sig = ",float(sig[i]), "spike_threshold ", float(spike_threshold))
          else:
             spiketrain_overthresh_bool[i] = 0
             #print(" under threshold sample, sig = ",float(sig[i]), "spike_threshold ", float(spike_threshold))
  
      if i > 0:
          if int(spiketrain_overthresh_bool[i]) > int(spiketrain_overthresh_bool[i-1]) and i > (starts[-1] + fs/max_spike_freq): # !!!!!!!! Reject spike if too soon after previous.

             starts.append(i)
             #print("*Spike Detected ")
             #is burst?
          if spiketrain_overthresh_bool[i-1] > spiketrain_overthresh_bool[i]:
             stops.append(i)
             
   #Generate spike widths...
   starts_stops_length = int(len(starts))
   spike_widths = []
   for k in range(0,starts_stops_length-1):
       spike_widths = np.append(spike_widths, stops[k] - starts[k])
   n_spikes = len(starts)
   #print("number of spikes detected",n_spikes)
   end_time = time.time()
   time_elapsed = end_time - start_time
   print("time elapsed detecting regular spikes",time_elapsed)
   #remove first start of 0:
   #starts.pop(0)
   return(starts, stops, spike_widths, n_spikes) #units of starts, stops, spike_widths = SAMPLES

def generate_starts_stops2(sig, threshold): #detects spikes by slope rather than threshold. Takes RAW signal with stim artefact removed.
    #only works for downward deflecting spikes currently..........
    spike_starts = [0] #units: samples.
    i = 0
    siglength = len(sig)
    for i in range(Slope_dt, siglength): # i = [samples]
        slope, intercept, r, p, se = linregress([i-Slope_dt, i], [sig[i-Slope_dt],sig[i]]) #look back.
        if slope <= threshold and i > (spike_starts[-1] + fs/max_spike_freq):
            spike_starts.append(i)
    #print("*****spike starts", spike_starts)
    return spike_starts    
      
def stim_train_detect(sig, stim_cut):     #returns list of spike starts and stops, in samples.
    start_time = time.time()
    stim_starts = []
    stim_stops = []
    i = 0
    siglength = len(sig)
    #print("stim_train_detect: sig length",siglength)
    #print("sig shape",np.shape(sig))
    #print("sig sample",sig[0:10])
    stimtrain_overthresh_bool = [0] * siglength
    is_included = "no"
    # ======= Upward deflecting stim ====== At first sample over threshold, looks back for slope, if steep enough will count as stim spike.
    for i in range(siglength): # i = [samples]
          if float(sig[i]) >= float(stim_cut): stimtrain_overthresh_bool[i] = 1  # upward deflection
          else: stimtrain_overthresh_bool[i] = 0
          if i > 0:
              if (stimtrain_overthresh_bool[i]) > (stimtrain_overthresh_bool[i-1]): 
                 #check slope before adding start
                 slope, intercept, r, p, se = linregress([i-stim_slope_dt, i], [sig[i-stim_slope_dt],sig[i]])
                 #print("candidate starts slope", slope, "at ", sample_to_sec(i), "seconds")
                 if slope >= stim_slope_thresh:
                     stim_starts.append(i)
                     #print("*Stim artefact rising edge detected, slope ",round(slope,2), "at ",sample_to_sec(i), "secs")
                     is_included = "yes"
                 else:
                     is_included = "no"
              if stimtrain_overthresh_bool[i-1] > stimtrain_overthresh_bool[i]:
                 if is_included == "yes":
                     #print("stim stop at ", sample_to_sec(i))
                     #if slope <= -stim_slope_thresh:
                     stim_stops.append(i)
                     
                     
    starts_stops_length = int(len(stim_starts))
    stim_spike_widths = []
    for k in range(0,starts_stops_length-1): stim_spike_widths = np.append(stim_spike_widths, stim_stops[k] - stim_starts[k])
    stim_n_spikes = len(stim_starts)
    #print("number of stim spikes detected",stim_n_spikes, ", at locations", stim_starts)
    end_time = time.time()
    time_elapsed = end_time - start_time
    #print("time elapsed detecting stim spikes",time_elapsed)
    #print("stim starts",stim_starts)
    #print("stim stops",stim_stops)
    return(stim_starts, stim_stops, stim_spike_widths, stim_n_spikes)     

def remove_stim_artefacts(stim_starts, stim_stops, sig_data):
    #write zeros between stim start and stop
    stim_midpoints = []
    for i in range(len(stim_starts)): stim_midpoints.append((stim_starts[i] + stim_stops[i])/2)
    #stim_midpoints = [(i + j)/2 for i in stim_starts for j in stim_stops]
    #print("Stim times",stim_starts)
    if blank_refractory == "no":
        sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
        sig_blank_stops = [(i + (stim_blanking/2000)*fs) for i in stim_midpoints]
        for i, j in zip(sig_blank_starts, sig_blank_stops):
            sig_data[int(i):int(j)] = 0
    elif blank_refractory == "yes":
        sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
        sig_blank_stops = [(i + (stim_blanking/2000)*fs + (stim_refractory/1000)*fs) for i in stim_midpoints]
        for i, j in zip(sig_blank_starts, sig_blank_stops):
            sig_data[int(i):int(j)] = 0 #Writes zero to the signal data values within blanking intervals.
    stim_midpoints_scaled = [sample_to_sec(x) for x in stim_midpoints]
    print("stim artefacts removed at",stim_midpoints_scaled, " seconds")
    return sig_data, sig_blank_starts, sig_blank_stops, stim_midpoints
        
def remove_epsps(stim_starts, stim_stops, signal):
    #takes whole signal and subtracts low freq waveform proceeding stim artefact.
    # returns signal, list of starts, list of stops of epsp removal section
    #!!!!! TO DO: fix glitch at end of EPSP. Possible solution write a straight line with two endpoints instead of 0
    epsp_starts = []; epsp_stops = []
    #stim_midpoints = [] #New
    for i in range(len(stim_starts)):
        # print("remove epsp i",i)
        epsp_t_start = int(stim_stops[i] + (stim_refractory/1000)*fs) #[samples]
        epsp_t_stop = int(epsp_t_start + (epsp_length/1000)*fs) #[samples]
        epsp_starts.append(epsp_t_start)
        epsp_stops.append(epsp_t_stop)
        #print("EPSP start (secs)",sample_to_sec(epsp_t_start))
        #print("EPSP stop (secs)",sample_to_sec(epsp_t_stop))
        sig_epsp_waveform = low_pass_filter2(signal[epsp_t_start:epsp_t_stop], epsp_highcut, fs, order=2)
        #subtract above from whole signal between indices.
        signal[epsp_t_start:epsp_t_stop] = signal[epsp_t_start:epsp_t_stop] - sig_epsp_waveform
        '''spl = UnivariateSpline(x, y, w=~w)
        #Try univariate spline 5/26/21
        #https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline
        
        #Polyfit didn't work...
        #poly_coeffs = np.polyfit(range(epsp_t_start, epsp_t_stop), sig_epsp_waveform, poly_deg, rcond=None, full=False, w=None, cov=False)
        #print("poly coefficients",poly_coeffs)
        #epsp_poly_Waveform = np.poly1d(poly_coeffs)
        
        new_x = range(epsp_t_start, epsp_t_stop)
        new_y = spl(new_x)
        
        #Plot poly
        plt.figure(figsize=graph_size)
        title = "poly"
        plt.title(title)
        #plt.xlabel(t_display_units)
        #Plot neural data
        plt.plot(new_x, new_y, linewidth=1) # poly
        #print("len new_x",len(new_x))
        #print("len new_x",len(new_x))
        plt.scatter(new_x, sig_epsp_waveform, marker="D", c="Orange") # orig sig
        plt.show()
        plt.close()'''
        '''
        plt.figure(figsize=graph_size)
        title = "EPSP"
        plt.title(title)
        plt.xlabel(t_display_units)
        #Plot neural data
        plt.plot(range(epsp_t_start,epsp_t_stop), sig_epsp_waveform, linewidth=1) # # filtered signal'''
        
        '''
        #Refractory blanking if choice = auto:
        stim_midpoints.append((stim_starts[i] + stim_stops[i])/2)
        if blank_refractory == "auto":
            sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
            sig_blank_stops = [(i + (stim_blanking/2000)*fs + (stim_refractory/1000)*fs) for i in stim_midpoints]
            for i, j in zip(sig_blank_starts, sig_blank_stops):
                sig_data[int(i):int(j)] = 0
        '''
    return signal, epsp_starts, epsp_stops
'''
def stim_period(stim_mids): #Returns the mean period between stim spikes
    inter_stim_intervals = [0] * (len(stim_mids)-1)
    for i in range(len(stim_mids)-1):
        inter_stim_intervals[i] = stim_mids[i+1] - stim_mids[i]
    print("inter_stim_intervals",inter_stim_intervals)
    stim_period_ = sum(inter_stim_intervals)/len(inter_stim_intervals)
    return stim_period_'''

def Spike_baselines(raw_sig): #lookback from specified sample number to determine average signal value. Width in samples.
    width = baseline_width
    if spike_deflection == "down": Xtrema_index = spike_parameters_["local_min_t"]
    if spike_deflection == "up":  Xtrema_index = spike_parameters_["local_max_t"]
    skipback = Slope_dt #goes back this many samples from first spike extremum before starting lookback window (so that rising edge of spike isn't included in mean)
    baselines_y, baselines_t = [], []
    #Xtrema = Xtrema[1:]
    for Xtremum in Xtrema_index:
        baselines_y.append(stats.mean(raw_sig[(Xtremum - width - skipback):(Xtremum - skipback)])) #units mV
        baselines_t.append(Xtremum - skipback - width/2)
    spike_parameters_["baselines_t"] = baselines_t #ðŸ’¥ðŸ’¥ Samples, relative to start of t analyze window.
    spike_parameters_["baselines_y"] = baselines_y
    #print("baselines_y",spike_parameters_["baselines_y"])
    return baselines_t, baselines_y

def Find_local_extrema(raw_sig, starts):
    search_width = (fs/max_spike_freq)/3 #!!!! Adjust if looking for narrower/ wider spikes.
    #starts = starts[1:]
    local_max_t, local_max_y, local_min_t, local_min_y = [], [], [], []
    raw_sig = list(raw_sig)
    for start in starts:
        #print("start-search_width",start-search_width)
        sig_snippet = raw_sig[int(start-search_width):int(start+search_width)]
        local_max_value = max(sig_snippet) # units = [samples]
        local_min_value = min(sig_snippet) # units = [samples]
        max_index = sig_snippet.index(local_max_value) #gives index relative to start of scan window.  
        max_index = int(start - search_width + max_index) #absolute index in signal
        min_index = sig_snippet.index(local_min_value) #gives index relative to start of scan window.  
        min_index = int(start - search_width + min_index) #absolute index in signal
        local_max_t.append(int(max_index))
        local_max_y.append(raw_sig[max_index]) 
        local_min_t.append(int(min_index))
        local_min_y.append(raw_sig[min_index]) 
    #print("find local extrema debug: # of starts",len(starts),"len local min t",len(local_min_t))
    spike_parameters_["local_max_t"] = local_max_t
    spike_parameters_["local_max_y"] = local_max_y
    spike_parameters_["local_min_t"] = local_min_t
    spike_parameters_["local_min_y"] = local_min_y
    #print("local min t",local_min_t)
    #print("local max t",local_max_t)

def Spike_amplitudes():
    baselines_y = spike_parameters_["baselines_y"]
    minima_y = spike_parameters_["local_min_y"]
    maxima_y = spike_parameters_["local_max_y"]
    amplitudes = []
    for i in range(len(baselines_y)):
        if spike_deflection == "down": #amplitude = minimum - baseline.
            amplitudes.append(minima_y[i]-baselines_y[i])
        if spike_deflection == "up": #amplitude = minimum - baseline.
            amplitudes.append(maxima_y[i]-baselines_y[i])
    spike_parameters_["amplitudes"] = amplitudes
    #print("number of baselines_y",len(spike_parameters_["baselines_y"]))
    #print("number of spike amplitudes",len(spike_parameters_["amplitudes"]))
    #print("AMPLITUDES",amplitudes)
    return amplitudes

def Spike_rise_times(raw_sig): #creates in dict rise times in MILLISECONDS
    #Currently for falling slope only (downward deflecting spikes)
    spike_rise_times = []
    spike_start_times = []
    baselines_t = spike_parameters_["baselines_t"] 
    baselines_y = spike_parameters_["baselines_y"]
    local_min_t = spike_parameters_["local_min_t"] 
    for i in range(len(baselines_t)): #for each spike...
        added = False
        #BACKWARDS scan from local minimum to find first baseline sample... ðŸ˜ƒ <----
        scan_start_t = int(local_min_t[i])
        scan_end_t = int(baselines_t[i])-1
        for j in range(scan_start_t- scan_end_t):
            if raw_sig[scan_start_t - j] >= baselines_y[i]:
                spike_start_time = scan_start_t - j
                spike_start_times.append(spike_start_time) # for graphing.
                spike_rise_times.append(((local_min_t[i] - spike_start_time)/fs)*1000) #for reporting purposes, not graphing.
                added = True
                break
        if added == False:
            #print("WARNING: spike_rise_times function: spike start time and rise time failed to computer at ", str(sample_to_sec(baselines_t[i])), "secs")
            spike_start_times.append(int(baselines_t[i]))
            spike_rise_times.append("")

    #print("spike_start_times",spike_start_times)
    spike_parameters_["spike_rise_times"] = spike_rise_times 
    spike_parameters_["spike_start_times"] = spike_start_times
    '''print("**** Spike rise times debug:")
    print("length baselines_y",len(baselines_y))
    print("length baselines_t",len(baselines_t))
    print("length local_min_t",len(local_min_t))
    print("length spike_rise_times",len(spike_rise_times))
    print("length spike_start_times",len(spike_start_times))'''

def Half_widths(raw_sig):    #  !!!!!! RETURNS HALF-WIDTHS IN MILLISECONDS.  Works for downward deflecting spikes only .
    # ====== SET PARAMETER: ======
    decay_slope_scan_range = 15 #for half maximum: time in samples to search decay slope  (from minimum) 
    baselines_t = spike_parameters_["baselines_t"]
    baselines_y = spike_parameters_["baselines_y"]
    local_min_t = spike_parameters_["local_min_t"]
    local_min_y = spike_parameters_["local_min_y"]
    spike_start_times = spike_parameters_["spike_start_times"]
    '''print("**** Half widths debug:")
    print("length baselines_y",len(baselines_y))
    print("length baselines_t",len(baselines_t))
    print("length local_min_t",len(local_min_t))
    print("length local_min_y",len(local_min_y))
    print("length spike_start_times",len(spike_start_times))'''

    LHEs, RHEs, LHEs_y, RHEs_y, half_widths = [], [], [], [], []
    '''print("spike start times",spike_start_times)
    print("baselines_t",baselines_t)   
    print("local_min_t",local_min_t)'''
    ''' ===== Find nearest samples to half-maximum : ============================================='''
    for i in range(len(spike_start_times)): #for each spike...
        problem = False
        half_width_level = (baselines_y[i] + local_min_y[i]) / 2
        #half_width_y.append(half_width_level)
        absolute_difference_function = lambda list_value : abs(list_value - half_width_level)
        # ===== Find Left Hand Edge: Scan between spike start and local minimum. ===== 
        scan_start_t = spike_start_times[i]
        scan_end_t = local_min_t[i]
        #print("start",scan_start_t, " end",scan_end_t, " min", local_min_t[i])
        #print(raw_sig[scan_start_t:scan_end_t])
        try: nearest_neighbor_y = min(raw_sig[scan_start_t:scan_end_t], key=absolute_difference_function) #if there are two identical values that are closest to half-width y value, the first one will be returned.
        except: nearest_neighbor_y = 0
        raw_sig_list = raw_sig[scan_start_t:scan_end_t].tolist()
        if nearest_neighbor_y != 0: LHE = scan_start_t + raw_sig_list.index(nearest_neighbor_y)
        else: 
            LHE = 0; problem = True
            #print("WARNING: Error detected half-width left hand edge at", sample_to_sec(scan_start_t))
        LHEs.append(LHE); LHEs_y.append(raw_sig[LHE])
        # ===== Find Right Hand Edge: Scan between local minimum and local minimum +  decay_slope_scan_range. ===== 
        scan_start_t = int(local_min_t[i])
        scan_end_t = scan_start_t + decay_slope_scan_range
        nearest_neighbor_y = min(raw_sig[scan_start_t:scan_end_t], key=absolute_difference_function) #if there are two identical values that are closest to half-width y value, the first one will be returned.
        raw_sig_list = raw_sig[scan_start_t:scan_end_t].tolist()
        RHE = scan_start_t + raw_sig_list.index(nearest_neighbor_y)
        RHEs.append(RHE); RHEs_y.append(raw_sig[RHE])
        #Calculate half widths and convert to milliseconds:
        if problem == False: half_widths.append(((RHE - LHE)/fs)*1000)
        else: half_widths.append("")
        
    spike_parameters_["half_width_LHEs"] = LHEs #for graphing purposes
    spike_parameters_["half_width_LHEs_y"] = LHEs_y #for graphing purposes
    spike_parameters_["half_width_RHEs"] = RHEs #for graphing purposes
    spike_parameters_["half_width_RHEs_y"] = RHEs_y #for graphing purposes
    spike_parameters_["half_widths"] = half_widths # for reporting purposes.
    #print("half widths [ms]", half_widths)
    #print("half widths LHEs", LHEs)
    
def Spike_stats(stim_mids, starts): #returns spike_id (which stim spike the spike proceeds), and isi_list
    #Generate spike ID's...:
    #append stim_mids with a virtual extra stim to categorize "post" spikes
    stim_mids2=stim_mids
    if len(stim_mids) > 1: #If there's more than one stim. If there isn't, it's probably a baseline trial ;)
        stim_mids2.append(stim_mids[-1]+(stim_mids[-1]-stim_mids[-2])) #adds an extra category for post stim train spikes, which start one inter-stim interval after last spike.
    #if len(stim_mids) > 0: #Occasionally stim artefact not detected or there is none.
        spike_ids = np.digitize(starts, stim_mids2, False) #Spike ID 0 means pre-stim spike. Assigns IDs to spikes based on which stim pulse they proceed                    
    else:
        spike_ids = np.zeros(len(starts), dtype=int).tolist()
    #print("Spike IDs",spike_ids)
    #ISI Calculations: each spike except the first has a corresponding ISI.
    isi_list = [0] * (len(starts))
    for i in range(len(isi_list)-1):
        isi_list[i+1] = (starts[i+1] - starts[i])/fs
    spikes_per_stim = [0] * (len(stim_mids2)+1)
    for i in range(0,int(max(spike_ids)+1)):
        j = i
        for ID in spike_ids:
            if ID == j:
                spikes_per_stim[i] += 1    
    
    #spikes_per_stim[i] -= 1    #Adjust 
    if len(spike_ids) == 0: #If no spikes were detected, need spike_ids to be not empty.
        spike_ids = 0    

    #Av. ISI per stim calculation:
    #for each spike ID, make list of ISIs and find the mean and sd of the list
    av_isi_per_stim, sd_isi_per_stim = [], [] #len n_stim_spikes+2 (inc pre und post)
    isi_indices = []
    #isi_per_stim = []
    #print("######### Length stim mids",len(stim_mids))
    #print("#########  stim mids", stim_mids)

    if len(stim_mids) > 0:
        for j in range(len(stim_mids)+1):
            isi_per_stim = []
            isi_indices = []
            for i in range(len(spike_ids)):
                if spike_ids[i] == j:
                    isi_indices.append(i)
            [isi_per_stim.append(isi_list[k]) for k in isi_indices if isi_list[k] != 0]
            try:
                #print("Debug, ISIs per stim #", j, " ",isi_per_stim)
                av_isi_per_stim.append(stats.mean(isi_per_stim))
            except: #If there are no ISIs in this bin?
                av_isi_per_stim.append(0)
            try:
                sd_isi_per_stim.append(stats.stdev(isi_per_stim))
            except: #If there are no ISIs in this bin?
                sd_isi_per_stim.append(0)
    else:
        sd_isi_per_stim = stats.stdev(isi_list[1:])
        av_isi_per_stim = stats.mean(isi_list[1:])
    #print("av_isi_per_stim",av_isi_per_stim)
    #print("sd_isi_per_stim",sd_isi_per_stim)
    spike_stats_["sd_isi_per_stim"] = sd_isi_per_stim
    spike_stats_["av_isi_per_stim"] = av_isi_per_stim

    '''========== Spike bins: ============='''
    start_t_bins = np.arange(0, math.ceil(sample_to_sec(max(starts)))+1)
    #print("spike t bins",start_t_bins)
    starts_sec = [sample_to_sec(starts[i]) for i in range(len(starts))]
    starts_bin_counts = np.digitize(starts_sec, start_t_bins, True) 
    #print("starts_bin_counts",starts_bin_counts)
    starts_per_bin = [0] * (max(starts_bin_counts)+1)
    for i in range(max(starts_bin_counts)+1):
        for ID in starts_bin_counts:
            if ID == i:
                starts_per_bin[i] += 1    
    #print("starts_per_bin",starts_per_bin)
    starts_per_bin[0] -= 1 #Adjust first starts per bin bcoz it one too many
    spike_stats_["start_t_bins"] = start_t_bins
    spike_stats_["starts_per_bin"] = starts_per_bin

    #   ISIs per bin:  ==============
    isi_per_bin = []
    ticker = 0
    for i in range(len(starts_per_bin)): #other method: use bin counts as index iterator.
        isi_start_index = ticker + 1
        isi_end_index = ticker + starts_per_bin[i] + 1
        #print("DEBUG: isi_start_index", isi_start_index, "; isi_end_index", isi_end_index, "; ticker", ticker)
        #print("DEBUG: starts per bin", starts_per_bin[i], "isi_list: ", isi_list[isi_start_index:isi_end_index])
        try:        
            av_isi = stats.mean(isi_list[isi_start_index:isi_end_index])
        except:
            av_isi = 0 #If there's no spikes in this bin
        ticker += starts_per_bin[i]
        isi_per_bin.append(av_isi)
    #print("ISI PER BIN",isi_per_bin)
    spike_stats_["isi_per_bin"] = isi_per_bin

    '''========== Bursting analysis: ============='''
    isi_list_arr = np.array(isi_list)
    is_burst = isi_list_arr <= (1/min_burst_freq) #initial identification of which spikes are part of a burst, excluding the first spike of a burst.
    burst_ID = [0] * len(is_burst)
    #print("is_burst",is_burst)
    burst_number = 0
    #Len(n_spikes) = len(spike_ids) = len(burst_ID).
    #Assign burst IDs to each spike (0 means es no part of burst)
    for i in range(1,len(is_burst)-1):
        if is_burst[i] == 1 and is_burst[i-1] == 0: #Start of a burst
            burst_number += 1 
            burst_ID[i-1] = burst_number
            burst_ID[i] = burst_number
        if is_burst[i] == 1: #middle of a burst? 
            burst_ID[i] = burst_number
        if is_burst[i] == 1 and is_burst[i+1] == 0: #End of burst
            #print("END OF BURST")
            burst_ID[i] = burst_number
        #print("burst_number", burst_number)
    #Handling last spike:
    if is_burst[-1] == 1:
        #print("last thing is 1")
        if is_burst[-2] == 0:
            #print("second to last thing is 0")
            burst_ID[-1] = burst_number 
            burst_ID[-2] = burst_number
        elif is_burst[-2] == 1:
            burst_ID[-1] = burst_number
    #print("is burst",is_burst)
    #print("burst_ID",burst_ID)
    n_bursts = burst_number #nice
    #print("n bursts",n_bursts)
    spikes_per_burst  = []
    for j in range(1,n_bursts+1):  #If first number in range is 0, then first number in list will be the number of zeros in burst_ID (i.e. number of spikes that are not part of a burst)
        spikes_per_burst.append(burst_ID.count(j))
    #Burst durations: time of last spike - time of first spike (= sum of ISIs except the first)
    burst_durations = []
    for i in range(1,n_bursts+1):#i: for each burst, burst index
        burst_index = []
        for j in range(len(burst_ID)): #j: scan index for entire burst_ID list. this method seems inefficient...
            if burst_ID[j] == i:
                burst_index.append(j) #creates list of indices of where burst is occurring.
        burst_start_index = burst_index[0]
        burst_end_index = burst_index[-1]
        #print("burst_start_index and value",burst_start_index, starts[burst_start_index])
        #print("burst_end_index and value",burst_end_index, starts[burst_end_index])
        burst_durations.append(sample_to_sec(starts[burst_end_index] - starts[burst_start_index])) #[seconds]        
    
    #interburst spikes:
    IBI_start = 0
    if n_bursts > 1:
        IBI_dur, IBI_n_spikes, IBI_av_ISIs, IBI_ISIs = [], [], [], [] #returned list
        for j in range(len(burst_ID)-1): #writew IBI_n_spikes and IBI_av_ISIs for each burst ID
            IBI_av_ISI = []
            
            #lookback to check if IBI start
            if burst_ID[j] == 0 and burst_ID[j-1] != 0 and j != 0: # start new interburst when previous spike was part of a burst
                #print("start new IBI")
                IBI_start = j-1 
            elif burst_ID[j] != burst_ID[j-1]: #if one burst proceeds the previous with no IBI spikes......
                IBI_start = j-1
                
            #look forward to see if it is IBI end
            if burst_ID[j] == 0 and burst_ID[j+1] != 0: #end of IBI reached, do some calculations on it
                IBI_end = j+1 #index
                #print("IBI:",IBI_start, IBI_enccd)
                #Calculate interburst interval in seconds:
                IBI_t = sample_to_sec(starts[IBI_end] - starts[IBI_start])
                IBI_dur.append(IBI_t)
                    
                IBI_n_spikes.append(IBI_end - IBI_start - 1)
                #IBI ISI start at IBI_start+1 and end at IBI_end - 1
                #IBI_av_ISI = IBI_t / (IBI_end - IBI_start - 1) #<-- old way
                if IBI_end - IBI_start - 1 > 0: 
                    for i in range(IBI_start+1,IBI_end+1):
                        IBI_av_ISI.append(isi_list[i])
                        IBI_ISIs.append(isi_list[i])
                        #print("IBI ISI",isi_list[i])
                    #IBI_sd_ISI = stats.stdev(IBI_av_ISI)
                    IBI_av_ISI = stats.mean(IBI_av_ISI)
                    IBI_av_ISIs.append(IBI_av_ISI)
                    #IBI_sd_ISIs.append(IBI_sd_ISI)
                else:
                    IBI_av_ISIs.append(0)
                    #IBI_sd_ISIs.append(0)
            if burst_ID[j] != burst_ID[j+1] and burst_ID[j] != 0 and burst_ID[j+1] != 0: #one burst follows immediately after the next.
                    IBI_start = j-1
                    IBI_end = j 
                    IBI_t = sample_to_sec(starts[IBI_end] - starts[IBI_start])
                    IBI_dur.append(IBI_t)
                    IBI_n_spikes.append(0)
                    IBI_av_ISIs.append("")
                    #IBI_sd_ISIs.append("")
    else:
        IBI_dur, IBI_n_spikes = [0], [0] #returned list
        IBI_av_ISIs = [""] #returned list
        #IBI_sd_ISIs = [0] #returned list
    try: av_ibi_n_spikes = stats.mean(IBI_n_spikes[1:]) #First row of IBI stats is ignored - becoz it meaningless.
    except: av_ibi_n_spikes = 0
    #print("IBI DUR", IBI_dur)
    #print("IBI DUR trimmed", IBI_dur[1:])
    try: av_IBI_dur = stats.mean(IBI_dur[1:])
    except: av_IBI_dur = 0
    try: mean_IBI_ISI = stats.mean(IBI_ISIs)
    except: mean_IBI_ISI = 0
    spike_stats_["av_ibi_n_spikes"] = av_ibi_n_spikes
    spike_stats_["av_IBI_dur"] = av_IBI_dur
    spike_stats_["mean_IBI_ISI"] = mean_IBI_ISI
    #end interburst stuff        
    spikes_per_burst_ = [hahaha -1 for hahaha in spikes_per_burst]
    av_burst_spike_freq = np.divide(spikes_per_burst_, burst_durations) #[Hz]
    if av_burst_spike_freq.size > 0:
        mean_burst_freq = sum(av_burst_spike_freq)/len(av_burst_spike_freq)
    else:
        mean_burst_freq = 0
    if av_burst_spike_freq.size > 1:
        sd_burst_spike_freq = stats.stdev(av_burst_spike_freq)
    else:
        sd_burst_spike_freq = 0
    #print("mean_burst_freq",mean_burst_freq)
    #print("n bursts",n_bursts)
    #print("n spikes per burst",spikes_per_burst)
    #print("burst durations",burst_durations)
    #print("av_av_burst_spike_freq",av_burst_spike_freq)    #lists with length 
    #Write spike_stats_ entries with data:
    spike_stats_["spike_ids"] = spike_ids #len n_spikes
    spike_stats_["isi_list"] = isi_list #len n_spikes
    spike_stats_["spikes_per_stim"] = spikes_per_stim #len n_stim_spikes + 2 (pre & post)
    spike_stats_["av_isi_per_stim"] = av_isi_per_stim #len 1 for pre or post, or n_stim_spikes for stim trial.
    spike_stats_["sd_isi_per_stim"] = sd_isi_per_stim #len 1
    #Burst stats:
    spike_stats_["burst_ID"] = burst_ID #len n_spikes
    spike_stats_["mean_burst_freq"] = mean_burst_freq #len 1
    spike_stats_["n_bursts"] = n_bursts #len 1
    spike_stats_["spikes_per_burst"] = spikes_per_burst #len n_bursts
    spike_stats_["burst_durations"] = burst_durations #len n_bursts. [samples]
    spike_stats_["av_burst_spike_freq"] = av_burst_spike_freq #len n_bursts
    spike_stats_["sd_burst_spike_freq"] = sd_burst_spike_freq #len n_bursts
    spike_stats_["IBI_n_spikes"] = IBI_n_spikes #len n_IBIs
    spike_stats_["IBI_av_ISIs"] = IBI_av_ISIs #len n_IBIs
    #spike_stats_["IBI_sd_ISIs"] = IBI_sd_ISIs #len n_IBIs
    spike_stats_["IBI_dur"] = IBI_dur #len n_IBIs
    
    return spike_ids, isi_list, spikes_per_stim, av_isi_per_stim, sd_isi_per_stim, burst_ID
    #Len(n_spikes) = len(spike_ids) = len(burst_ID).
    #spike_ids: burst_ID: Burst ID assignment for each spike.

def write_trial_worksheet(starts, stim_starts, ISIs, spike_id, threshold, info, is_burst): #Per input file writes an xlsx file containing list of ISI's, basic stats and filtered signal.
    #Writes a single worksheet.   
    #Workbook must already be open. 
    print("Writing Excel file.... ")
    '''if spike_id[0] == 0: #Need to fix this method, doesn't work if some stims don't have any spikes after them. Instead generate ID's by comparing each stim time to spike time.
        stim_sheet_indices = []
    else:
        stim_sheet_indices = [0]
    for i in range(len(spike_id)):
        try:
            if spike_id[i] != spike_id[i+1]:
                stim_sheet_indices.append(i)
        except:
            pass
    stim_sheet_indices = [i + 2 for i in stim_sheet_indices]'''
    if len(stim_starts) == 0: stim_starts.append(0)
    stim_sheet_indices = range(1,len(stim_starts)+1) #!!!!!!Need to align start times with spike times row number.
    worksheet = workbook.add_worksheet(trial_tags[trial_number].split(" ")[2] + " " + str(trial_number))
    col_title = workbook.add_format(); col_title.set_text_wrap()
    col_title.set_bottom(); col_title.set_bold()
    #col_title.set_italic()
    cell_right_border = workbook.add_format(); cell_right_border.set_right()
    cell_title = workbook.add_format(); cell_title.set_bold()
    col_title_border = workbook.add_format(); col_title_border.set_right(); col_title_border.set_bottom()
    col_title_border.set_text_wrap(); col_title_border.set_bold()
    #stim_border = workbook.add_format()
    #col_title_border.set_italic()
    worksheet.write(0,0, "Stim time [s]", col_title)
    worksheet.write(0,1, "Spike time [s]", col_title)
    worksheet.write(0,2, "Stim Spike #", col_title) 
    worksheet.write(0,3, "Amplitude [mV]", col_title)
    worksheet.write(0,4, "Rise time [ms]", col_title) 
    worksheet.write(0,5, "Half-width [ms]", col_title) 
    worksheet.write(0,6, "ISI [ms]", col_title) 
    worksheet.write(0,7, "Burst ID,", col_title_border) 
    worksheet.write(0,9, "Burst #", col_title)
    worksheet.write(0,10, "n spikes/ burst", col_title) 
    worksheet.write(0,11, "Burst durations [ms]", col_title)
    worksheet.write(0,12, "Av. burst spike freq [Hz]", col_title) #updated etc... +3 for the rest too
    #worksheet.write(0,10, "Ïƒ burst spike freq [Hz]", col_title)
    worksheet.write(0,13, " ", col_title)
    worksheet.write(0,14, "IBI n spikes", col_title)
    worksheet.write(0,15, "IBI av ISI [ms]", col_title)
    #worksheet.write(0,13, "IBI Ïƒ ISI [s]", col_title)
    worksheet.write(0,16, "IBI duration [ms]", col_title_border)
    worksheet.write(1,17, " ")

    if len(stim_starts) > 1: #If it a stim trial...
        worksheet.write(0,19, "Stim #",col_title)
        worksheet.write(0,20, "n spikes/ stim", col_title)
        worksheet.write(0,21, "Av. ISI/ stim [ms]", col_title)
        worksheet.write(0,22, "Ïƒ ISI/ stim [ms]", col_title_border)
    else:
        worksheet.write(0,19, "Baseline Trial",col_title)
        worksheet.write(0,20, "n spikes", col_title)
        worksheet.write(0,21, "Av. ISI", col_title)
        worksheet.write(0,22, "Ïƒ ISI", col_title_border)
    worksheet.write(0,24, "Spike t bin (right edge)[s]", col_title)
    worksheet.write(0,25, "Bin Counts", col_title)
    worksheet.write(0,26, "Av. ISI/ bin [ms]", col_title_border)
    worksheet.write(0,28, "PARAM INFO:",col_title)
    worksheet.write(1,28, info)
    worksheet.write(3,28, "IBI = Interburst interval [ms]")
    worksheet.write(4,28, "IBIs precede corresponding burst")
    worksheet.write(6,28, "TRIAL STATS:",cell_title)
    worksheet.write(9,28, "mean burst freq [Hz]",cell_title)
    worksheet.write(10,28, spike_stats_["mean_burst_freq"])
    worksheet.write(11,28, "Ïƒ burst spike freq [Hz]",cell_title)
    worksheet.write(12,28, spike_stats_["sd_burst_spike_freq"])
    worksheet.write(7,28, "n bursts",cell_title)
    worksheet.write(8,28, spike_stats_["n_bursts"])
    worksheet.write(13,28, "Av. IBI n spikes",cell_title)
    worksheet.write(14,28, spike_stats_["av_ibi_n_spikes"])
    worksheet.write(15,28, "Av. IBI duration [ms]",cell_title)
    worksheet.write(16,28, spike_stats_["av_IBI_dur"])
    worksheet.write(17,28, "Av. IBI ISI [ms]",cell_title)
    worksheet.write(18,28, spike_stats_["mean_IBI_ISI"]*1000) 
    worksheet.write(20,28, "n spikes", cell_title)
    worksheet.write(21,28, len(spike_parameters_["starts_included"]))
    for i in range(len(stim_starts)):
        worksheet.write(stim_sheet_indices[i],0, stim_starts[i], cell_right_border)
    [worksheet.write(i+1, 1 , sample_to_sec(spike_parameters_["starts_included"][i]), cell_title) for i in range(len(spike_parameters_["starts_included"]))]
    if max(spike_id) > 0: [worksheet.write(i+1, 2 , spike_id[i]) for i in range(len(spike_id))]
    [worksheet.write(i+1, 3 , spike_parameters_["amplitudes_included"][i]) for i in range(len(spike_parameters_["amplitudes_included"]))] #new!!
    [worksheet.write(i+1, 4 , spike_parameters_["rise_times_included"][i]) for i in range(len(spike_parameters_["rise_times_included"]))] #new!!
    [worksheet.write(i+1, 5 , spike_parameters_["half_widths_included"][i]) for i in range(len(spike_parameters_["half_widths_included"]))] #new!!
    [worksheet.write(i+1, 6 , ISIs[i]*1000) for i in range(len(ISIs))]
    [worksheet.write(i+1, 7 , is_burst[i], cell_right_border) for i in range(len(is_burst))]
    [worksheet.write(i+1, 9 , i+1, cell_title) for i in range(spike_stats_["n_bursts"])]
    [worksheet.write(i+1, 10 , spike_stats_["spikes_per_burst"][i]) for i in range(len(spike_stats_["spikes_per_burst"]))] 
    [worksheet.write(i+1, 11 , spike_stats_["burst_durations"][i]*1000) for i in range(len(spike_stats_["burst_durations"]))]
    [worksheet.write(i+1, 12 , spike_stats_["av_burst_spike_freq"][i]) for i in range(len(spike_stats_["av_burst_spike_freq"]))]
    #[worksheet.write(i+1, 10 , spike_stats_["sd_burst_spike_freq"][i]) for i in range(len(spike_stats_["sd_burst_spike_freq"]))]
    if spike_stats_["n_bursts"] > 0:
        spike_stats_["IBI_n_spikes"][0] = ""  #For spreadsheet, replace first row of IBI stats with blank - these were not included in IBI stats calculations anyway...
        spike_stats_["IBI_av_ISIs"][0] = ""
        spike_stats_["IBI_dur"][0] = ""
    [worksheet.write(i+1, 14 , spike_stats_["IBI_n_spikes"][i]) for i in range(len(spike_stats_["IBI_n_spikes"]))]
    [worksheet.write(i+1, 15 , spike_stats_["IBI_av_ISIs"][i]*1000) for i in range(len(spike_stats_["IBI_av_ISIs"]))]
    #[worksheet.write(i+1, 13 , spike_stats_["IBI_sd_ISIs"][i]) for i in range(len(spike_stats_["IBI_sd_ISIs"]))]
    [worksheet.write(j+1, 16 , spike_stats_["IBI_dur"][j]*1000, cell_right_border) for j in range(len(spike_stats_["IBI_dur"]))]
    if len(stim_starts) < 2: #then it baseline trial.
        #spikes_per_stim.pop(0)
        if type(spike_stats_["av_isi_per_stim"]) == list:
            worksheet.write(i+1, 21 , spike_stats_["av_isi_per_stim"][0]*1000) 
            worksheet.write(i+1, 22 , spike_stats_["sd_isi_per_stim"][0]*1000, cell_right_border)
        else:
            worksheet.write(i+1, 21 , spike_stats_["av_isi_per_stim"]*1000) 
            worksheet.write(i+1, 22 , spike_stats_["sd_isi_per_stim"]*1000, cell_right_border) 
    else: #stim trial
        if len(stim_starts) > 1:
            worksheet.write(1,19, "pre ---->")
            worksheet.write(len(spikes_per_stim),19, "post ---->")
        if type((spike_stats_["av_isi_per_stim"])) == list:
            [worksheet.write(i+1, 21 , spike_stats_["av_isi_per_stim"][i]*1000) for i in range(len(spike_stats_["av_isi_per_stim"]))]            
        [worksheet.write(i+1, 22 , spike_stats_["sd_isi_per_stim"][i]*1000, cell_right_border) for i in range(len(spike_stats_["sd_isi_per_stim"]))]
    if len(spikes_per_stim) < 3: #baseline
        [worksheet.write(i+1, 20 , spikes_per_stim[0])] #spikes_per_stim is a global variable 
    else:
        [worksheet.write(i+1, 20 , spikes_per_stim[i]) for i in range(len(spikes_per_stim))] #spikes_per_stim is a global variable 
        [worksheet.write(i+1, 19 , i, cell_title) for i in range(len(spikes_per_stim))]
        if len(stim_starts) > 1:
            worksheet.write(1,19, "pre ---->")
            worksheet.write(len(spikes_per_stim),19, "post ---->")
    [worksheet.write(i+1, 24 , spike_stats_["start_t_bins"][i], cell_title) for i in range(1,len(spike_stats_["start_t_bins"]))]
    [worksheet.write(i+1, 25 , spike_stats_["starts_per_bin"][i]) for i in range(1,len(spike_stats_["starts_per_bin"]))]
    [worksheet.write(i+1, 26 , spike_stats_["isi_per_bin"][i]*1000, cell_right_border) for i in range(1,len(spike_stats_["isi_per_bin"]))]
    print("Trial " + str(trial_number) + " written")
    #End of write_trial_worksheet function

def write_summary_worksheet(info_):
    cell_title = workbook.add_format(); cell_title.set_bold()
    stim_title = workbook.add_format();     stim_title.set_bg_color('yellow');     stim_title.set_bold()
    section_title = workbook.add_format(); section_title.set_italic(); section_title.set_underline()
    summary_cell = workbook.add_format(); summary_cell.set_bg_color('#DBDBDB')
    #Set up key column and row numbers:
    key_title_list = ["Trial duration [s]", "N. spikes", "Av. ISI [ms]", "Ïƒ ISI [ms]", "N. bursts", "Av. burst spike f [Hz]", "Ïƒ burst spike f [Hz]", "Av. spikes/ burst", "Ïƒ spikes/ burst", "Max. spikes/ burst", "Av. IBI n spikes", "Av. IBI ISI [ms]", "Av. IBI duration [s]", "% spikes in bursts"]
    key_row_list = (1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 18)
    #worksheet.write format: (row, column, value)
    for i in range(len(key_title_list)): #writes left hand column text
        summary_worksheet.write(key_row_list[i], 0, key_title_list[i], cell_title)  

    for i in range(len(sum_["durations"])): #Write trial summary stats. All these things should be the same length. i iterates column number.
        summary_worksheet.set_column(0, i+1, 4) #===== Set Column width (in weird Excel units) =======
        summary_worksheet.write(key_row_list[0], i+1, sum_["durations"][i])
        summary_worksheet.write(key_row_list[1], i+1, sum_["M_n_spikes"][i])
        summary_worksheet.write(key_row_list[2], i+1, round(sum_["M_av_isi"][i], 1))
        summary_worksheet.write(key_row_list[3], i+1, round(sum_["M_sd_isi"][i], 1))
        summary_worksheet.write(key_row_list[4], i+1, round(sum_["M_n_bursts"][i], 1))
        summary_worksheet.write(key_row_list[5], i+1, round(sum_["M_av_burst_f"][i], 1))
        summary_worksheet.write(key_row_list[6], i+1, round(sum_["M_sd_burst_f"][i], 1))
        summary_worksheet.write(key_row_list[7], i+1, round(sum_["M_av_spike_per_burst"][i], 1))
        summary_worksheet.write(key_row_list[8], i+1, round(sum_["M_sd_spike_per_burst"][i], 1))
        summary_worksheet.write(key_row_list[9], i+1, round(sum_["M_max_spike_per_burst"][i], 1))
        summary_worksheet.write(key_row_list[10], i+1, round(sum_["M_av_IBI_n_spikes"][i], 2))
        summary_worksheet.write(key_row_list[11], i+1, round(sum_["M_av_IBI_ISI"][i], 1))
        summary_worksheet.write(key_row_list[12], i+1, round(sum_["M_av_IBI_dur"][i], 1))
        summary_worksheet.write(key_row_list[13], i+1, round(sum_["M_%_spikes_in_bursts"][i], 1))
    if pre_n_trials > 0: #Writes post trial  stats.
        for i in range(pre_n_trials):
            summary_worksheet.write(0, i+1, ("Pre #"+str(i+1)), cell_title)
    summary_worksheet.write(0, pre_n_trials+1, "Stim", stim_title) #Writes stim  stats.
    if post_n_trials > 0: #Writes post trial  stats.
        for i in range(post_n_trials):
            summary_worksheet.write(0, (pre_n_trials+2+i), ("Post #"+str(i+1)), cell_title)
    summary_worksheet.write(0, 0, "Trial Summary:", section_title)
    summary_worksheet.set_column(0, 0, 21)

    #======= Experiment summary stats (at bottom): ==========
    summary_worksheet.write(20, 0, "Experiment Summary:", section_title)
    summary_worksheet.write(21, 1, "Pre", cell_title)
    summary_worksheet.write(21, 2, "Post", cell_title)
    #Dictionary Keys: exp summary variables:
    key_sum_Title_list = ["Av. n spikes", "Ïƒ n spikes", "Av. ISI [ms] (w)", "Ïƒ ISI [s] (w)", "Av. n bursts", "Ïƒ n bursts",
    "Av. burst f [Hz] (w)", "Ïƒ burst f [Hz] (w)", "Av. spikes/burst (w)","Ïƒ spikes/burst (w)","Av. IBI n spikes (w)","Ïƒ IBI n spikes (w)",
    "Av. IBI ISI [ms] (w)","Ïƒ IBI ISI [ms] (w)","Av. IBI duration [ms] (w)", "Ïƒ IBI duration [ms] (w)"]
    key_sum_row_list = [23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40]
    key_sum_var_list = ["Sum_av_n_spikes", "Sum_sd_n_spikes", "Sum_av_av_ISI", "Sum_sd_av_ISI", "Sum_av_n_bursts", "Sum_sd_n_bursts",
    "Sum_av_av_burst_f", "Sum_sd_av_burst_f", "Sum_av_av_spikes_per_burst","Sum_sd_av_spikes_per_burst","Sum_av_av_IBI_n_spikes","Sum_sd_av_IBI_n_spikes",
    "Sum_av_av_IBI_ISI","Sum_sd_av_IBI_ISI","Sum_av_av_IBI_dur","Sum_sd_av_IBI_dur"]
    for i in range(len(key_sum_Title_list)): 
        summary_worksheet.write(key_sum_row_list[i]-1, 0, key_sum_Title_list[i], cell_title) #writes left hand column text
        #write pre and post  values:
        summary_worksheet.write(key_sum_row_list[i]-1, 1, sum_[key_sum_var_list[i]][0], summary_cell) 
        summary_worksheet.write(key_sum_row_list[i]-1, 2, sum_[key_sum_var_list[i]][1], summary_cell) 
    
    #=== info at very bottom:
    summary_worksheet.write(42, 0, "Parameter Info:", section_title)
    summary_worksheet.write(43, 0, info)


def Reject_spikes(starts): #Reject based on amplitude, rise time or half-width.
    # Then re-reject based on max frequency.
    #!!!!!! Need to add reject by rise time or half width.
    reject_count = 0
    starts_freq_filtered = [starts[0]] #**
    is_included = [0] * (len(starts))
    for i in range(len(starts)):
        if spike_parameters_["amplitudes"][i] >= Amplitude: #downward deflecting spikes only.
            is_included[i] = 0 #Spike rejected.
            #print("rejected: time", sample_to_sec(starts[i]),"amp: ",spike_parameters_["amplitudes"][i])
        else:
            if starts[i] - starts_freq_filtered[-1] >= fs/max_spike_freq:
                starts_freq_filtered.append(starts_final[i])
                is_included[i] = 1
            else:
                reject_count += 1
    print("Number of spikes rejected by max. freq. filter: ",reject_count)
            #print("** accepted: time", sample_to_sec(starts[i]),"amp: ",spike_parameters_["amplitudes"][i])
    #print("is included",is_included)
    starts_included, amplitudes_included, rise_times_included, half_widths_included, local_min_t_included, local_min_y_included = [], [], [], [], [],[]
    '''print("** Reject spikes debug... before rejection")
    print("length amplitudes]", len(spike_parameters_["amplitudes"]))
    print("length spike_rise_times]", len(spike_parameters_["spike_rise_times"]))
    print("length half_widths]", len(spike_parameters_["half_widths"]))'''
    
    # =====  REAPPLY MAX SPIKE FREQ FILTER after rejecting spikes. =========================================================
    '''    
    starts_freq_filtered = [starts_final[0]]
    for i in range(1,len(starts_final)):
        if starts_final[i] - starts_freq_filtered[-1] >= fs/max_spike_freq:
            starts_freq_filtered.append(starts_final[i])
        else:
            #print("** Spike rejected by max. freq. filter at ", sample_to_sec(starts_final[i]))
            reject_count += 1
            print(sample_to_sec(starts_freq_filtered[-1]))
    starts_final = starts_freq_filtered'''       

    # ========================
    
    for i in range(len(starts)):
        if is_included[i] == 1:
            starts_included.append(starts[i])
            amplitudes_included.append(spike_parameters_["amplitudes"][i])
            rise_times_included.append(spike_parameters_["spike_rise_times"][i])
            half_widths_included.append(spike_parameters_["half_widths"][i])
            local_min_t_included.append(spike_parameters_["local_min_t"][i])
            local_min_y_included.append(spike_parameters_["local_min_y"][i])
    spike_parameters_["starts_included"], spike_parameters_["amplitudes_included"] = starts_included, amplitudes_included
    spike_parameters_["rise_times_included"] = rise_times_included
    spike_parameters_["half_widths_included"] = half_widths_included
    spike_parameters_["local_min_t_included"] = local_min_t_included
    #print("local_min_t_included",local_min_t_included)
    spike_parameters_["local_min_y_included"] = local_min_y_included

    #print("starts included length",len(starts_included))
    #print(starts_included)
    return is_included #, a binary list of whether each start in starts_final is included in final analysis.

'''======================================
=======     END OF FUNCTIONS     ========
======================================'''
#Start running program..............
Run = "yes"
#!!!!! Add info to graphs. Threshold, filter, y-axis values and x-axis.
#!!!! Graph title =  Trial number and threshold.
start_time = time.time()

def start_of_run_loop():
    pass

#Create new directory to dump output files into...
thyme = time.asctime(); thyme =  thyme.replace(":","-")
directory = "Output " + thyme ; parent_dir = ""
path = os.path.join(parent_dir, directory)
os.mkdir(path)
directoryy = directory + "/"

while Run == "yes":

    for spike_threshold in Thresholds: #Create unique Excel worksheet and graphs for each threshold, for each Experiment.

        for experiment_num in range(len(stim_trial_numbers)):
            #Declare Master (cumulative) variables for experiment: used for cumulative statistical analysis (and to be implemented: auto stim detection.)
            dict_items = ["durations", "M_max_spike_per_burst", "M_stim_starts", "M_n_spikes", "M_av_isi", "M_sd_isi", "M_n_bursts", "M_av_burst_f", "M_sd_burst_f", 
                          "M_av_spike_per_burst", "M_sd_spike_per_burst", "M_max_spikes_burst", "M_av_IBI_n_spikes", "M_av_IBI_ISI", "M_av_IBI_dur", "M_%_spikes_in_bursts"]
            for name in dict_items: 
                sum_[name] = []
                #print("TYPE:", type(sum_[name]))
            #sum_["durations"] = sum_["M_max_spike_per_burst"] = sum_["M_stim_starts"] = sum_["M_n_spikes"] = sum_["M_av_isi"] = sum_["M_sd_isi"] = sum_["M_n_bursts"] = sum_["M_av_burst_f"] = sum_["M_sd_burst_f"] = sum_["M_av_spike_per_burst"] = sum_["M_sd_spike_per_burst"] = sum_["M_max_spikes_burst"] = sum_["M_av_IBI_n_spikes"] = sum_["M_av_IBI_ISI"] = sum_["M_av_IBI_dur"] = []
            #trial_numbers = []
            trial_tags = {} #Dictionary 
            x = stim_trial_numbers[experiment_num]
            if n_channels > 1: #if 2, use odd or even steps for trial numbers, if 1 use integer steps.
                pre_trials = np.arange(x-n_channels*pre_n_trials, x, n_channels).tolist()
                post_trials = np.arange(x+n_channels, x+n_channels*(post_n_trials+1), n_channels).tolist()

            #!!!!!!!!!!!! elif n_channels == 1........ 
            # trial_numbers.append(pre_trials)
            # trial_numbers.append(x) 
            # trial_numbers.append(post_trials)

            # ====  Create new Excel file for each Eggsperiment =====.
            if pre_n_trials != 0 and post_n_trials != 0:
                print("pre n trials and post n trials are not zero")
                print(pre_n_trials, post_n_trials)
                trial_numbers = pre_trials + [x] + post_trials
                bookname = directoryy + file[:-4] + " Exp " + str(experiment_num+1) + " (Trials " + str(pre_trials[0]) + "-" + str(post_trials[-1]) + ") Thresh " + str(spike_threshold) + " data.xlsx"
            elif pre_n_trials == 0 and post_n_trials != 0:
                trial_numbers = [x] + post_trials
                bookname = directoryy + file[:-4] + " Exp " + str(experiment_num+1) + " (Trials " + str(min(trial_numbers)) + "-" + str(max(trial_numbers)) + ") Thresh " + str(spike_threshold) + " data.xlsx"
            elif pre_n_trials != 0 and post_n_trials == 0:
                trial_numbers = pre_trials + [x]
                bookname = directoryy + file[:-4] + " Exp " + str(experiment_num+1) + " (Trials " + str(min(trial_numbers)) + "-" + str(max(trial_numbers)) + ") Thresh " + str(spike_threshold) + " data.xlsx"
            elif pre_n_trials == 0 and post_n_trials == 0:
                trial_numbers = [x]
                bookname = directoryy + file[:-4] + " Exp " + str(experiment_num+1) + " (Stim Trials " + str(min(trial_numbers)) + "-" + str(max(trial_numbers)) + ") Thresh " + str(spike_threshold) + " data.xlsx"
            workbook = xlsxwriter.Workbook(bookname)
            summary_worksheet = workbook.add_worksheet("Summary") #variable summary_worksheet is a worksheet object :D

            for y in pre_trials:
                trial_tags[y] = " Exp" + str(experiment_num+1) + " pre "
            trial_tags[x] = " Exp" + str(experiment_num+1) + " STIM "
            for y in post_trials:
                trial_tags[y] = " Exp" + str(experiment_num+1) + " post "
    
            for trial_number in trial_numbers:
                #One graph per trial. 
                #=======  Generate trial number  ============:
                print("Processing experiment number ",str(experiment_num+1), " trial number ",trial_number)
                trial_number = trial_number #It's true!
                
                #====== Pick trial to assign to master_data  ========= 
                #print("gap list", gap_list, "length", len(gap_list))
                if len(gap_list) > 1:
                    trial_data = data_float[(gap_list[trial_number]+1):(gap_list[trial_number+1])]
                else: #single trial mode.
                    trial_data = data_float
                master_data = np.array(trial_data)
                del trial_data
            
                #====== Trim data  =========           
                if t_analyze_window != "all": #Should the data be trimmed?
                    print("Data is being trimmed:")
                    if int(fs*t_analyze_window[0]) < len(master_data): 
                        t_startpoint = int(fs*t_analyze_window[0]) #[samples]
                        print("   t startpoint",t_startpoint, " samples")
                        if int(fs*t_analyze_window[1]) < len(master_data):
                            t_endpoint = int(fs*t_analyze_window[1]) #[samples]
                            print("   t_endpoint", t_endpoint, " samples")
                        else:
                            t_endpoint = int(len(master_data)) #[samples]
                    if t_startpoint > 0: #remove sample data from 0 to startpoint
                        trimmed_data = np.delete(master_data,np.s_[0:t_startpoint+1],axis=0)
                    else:
                        trimmed_data = master_data 
                    #trim from t_endpoint to end of data.
                    if len(master_data) > t_endpoint: #if data goes on beyond the specified end point time.
                        n_end_trim = len(master_data) - t_endpoint 
                        if n_end_trim > 0:
                            #trim from (last index - n_end_trim) to last index
                            #print("master data length",len(master_data))
                            #print("trimmed data shape",np.shape(trimmed_data))
                            n_row = int(np.shape(trimmed_data)[0])
                            #print("trimmed data type",type(trimmed_data))
                            #trimmed_data = np.delete(trimmed_data, 0, range(n_row - n_end_trim, n_row))
                            trimmed_data = np.delete(trimmed_data, np.s_[(n_row-n_end_trim): n_row],axis=0)
                else: #no trimming occurs.
                    t_startpoint = 0 #[samples]
                    if type(master_data) == list:
                        t_endpoint = int(len(master_data)) #[samples]
                    else:
                        t_endpoint = int(len(master_data)) #[samples]
                    #print("data not trimmed. t startpoint",t_startpoint)
                    #print("   t endpoint",t_endpoint)
                    trimmed_data = master_data 
                end_time = time.time()
                time_elapsed = end_time - start_time
                #print("time elapsed trimming data",time_elapsed)
                #print("t_endpoint [samples]: ",t_endpoint)
                y_values = trimmed_data
                del trimmed_data
                
                #==========  STIM AND EPSP PROCESSING  ===========
                raw_unblanked_sig = np.array(y_values)
                
                stim_starts, stim_stops, stim_widths, stim_n_spikes = stim_train_detect(y_values,stim_peak) # Generate stim_starts etc.
                if not trial_number in stim_trial_numbers: #if it not stim trial , we don't care about stim artefact ;)
                    stim_starts = [0]
                    stim_stops = [0]
                    stim_widths = [0]
                    stim_n_spikes = 1
                y_values, stim_blank_starts, stim_blank_stops, stim_midpoints = remove_stim_artefacts(stim_starts, stim_stops, y_values) # Remove stim artefact from signal
                y_values, epsp_starts, epsp_stops = remove_epsps(stim_starts, stim_stops, y_values) #remove EPSP from signal
                #epsp_starts = [0]
                #epsp_stops = [0]
                #  ========  UNIT SCALING, SAMPLES -> SECONDS: ================================= 
                print("Scaling data...")
                if t_analyze_window == "all":
                    trim_offset = 0
                else:
                    trim_offset = float(t_analyze_window[0])
        
                stim_starts_secs = [x/fs for x in stim_starts]
                stim_starts_scaled = [x+trim_offset for x in stim_starts_secs]
                stim_stops_secs = [x/fs for x in stim_stops]
                stim_stops_scaled = [x+trim_offset for x in stim_stops_secs]
                epsp_starts_scaled = [sample_to_sec(x) for x in epsp_starts]
                epsp_stops_scaled = [sample_to_sec(x) for x in epsp_stops]
                                
                # =========  APPLY FILTER   ============
                print("Applying filter...")
                def apply_filter():
                    pass
                if filter_type == "bandpass":
                    #print("band pass filter ",lowcut, highcut, "Hz")
                    filtered_signal = butter_bandpass_filter(y_values, lowcut, highcut, fs, order=filter_order) # filter is on
                elif filter_type == "highpass":
                    #print("high pass filter ",lowcut, "Hz")
                    filtered_signal = high_pass_filter(y_values, lowcut, fs, order=filter_order)
                elif filter_type == "lowpass":
                    #print("low pass filter ",lowcut, "Hz")
                    filtered_signal = low_pass_filter2(y_values, highcut, fs, order=filter_order)
                elif filter_type == "none":
                    #print("No filter applied")
                    filtered_signal = y_values
                else:
                    print("Filter type not entered correctly. Defaulting to #nofilter.")
                    filtered_signal = y_values
                
                #Write audio? 
                '''title = file[:-4] + " Trial " + str(trial_number) + " filtered"
                write(title + ".wav", fs, filtered_signal) #write data to a wav file'''
                        
                #  ========  SPIKE DETECTION & STATS: ===================================
                def call_spike_detection_algorithms():
                    pass
                # *****  PRIMARY SPIKE DETECTION  ***** :
                reject_count = 0
                print("Detecting spikes by slope...")
                starts2 = generate_starts_stops2(y_values,Slope)
                print("Detecting spikes by threshold...")
                starts, stops, widths, n_spikes = generate_starts_stops(filtered_signal,spike_threshold)
                print("Filtering spikes...")
                if primary_detection_mode == 1:
                    # COMBINE OVERLAPPING SPIKES FROM TWO LISTS, counting spikes that are very close together in time from each list as single event.
                    
                    spike_overlap_time = 0.3 #!!! [ms]. <----====== spike times (pooled from both detection methods) occurring together within this interval will be counted as one spike.
                    
                    starts_combined = starts + starts2
                    starts_combined.sort()   
                    #=====  Remove spikes that follows less than spike_overlap_time from previous...   =============
                    max_spike_freq_samples = fs/ max_spike_freq 
                    overlap_t_samples = (spike_overlap_time/1000) * fs #convert from milliseconds to samples.
                    indices_to_remove = [0] * len(starts_combined) #0 if keep, 1 to remove.
                    for i in range(1, len(starts_combined)):
                        #print(i, starts_combined[i-1])
                        if ((starts_combined[i] - starts_combined[i-1]) <= overlap_t_samples):
                            indices_to_remove[i] = 1
                        else:
                            indices_to_remove[i] = 0
                    starts_final = [0] * indices_to_remove.count(0)
                    #print("len starts final",len(starts_final))
                    counter = 0
                    #print("len indices_to_remove",len(indices_to_remove), "len starts_combined",len(starts_combined))
                    for i in range(len(indices_to_remove)):
                        if indices_to_remove[i] == 0:
                            starts_final[counter] = starts_combined[i] #Ends up deleting the Zero starts
                            counter += 1      
                    starts_final.pop(0)
                    #print("STARTS BEFORE FREQ FILTER: ",starts_final)
                    #print("number of spikes before freq filter", len(starts_final))
                    
                    '''# =====  REAPPLY MAX SPIKE FREQ FILTER after rejecting spikes. =========================================================
                    
                    starts_freq_filtered = [starts_final[0]]
                    for i in range(1,len(starts_final)):
                        if starts_final[i] - starts_freq_filtered[-1] >= fs/max_spike_freq:
                            starts_freq_filtered.append(starts_final[i])
                        else:
                            print("** Spike rejected by max. freq. filter at ", sample_to_sec(starts_final[i]))
                            reject_count += 1
                            print(sample_to_sec(starts_freq_filtered[-1]))
                    starts_final = starts_freq_filtered   '''     
                                        
                    #print("STARTS AFTER FREQ FILTER: ",starts_final)
                    #print("number of spikes after freq filter", len(starts_final))
                    #print("number of spikes rejected by freq filter", reject_count)
                print("Calculating spike parameters...")
                Find_local_extrema(y_values, starts_final) #takes starts list and generate dictionary items of local minima near starts.
                bl_x, bl_y = Spike_baselines(y_values) #takes local extrema dictionary values and finds nearby baseline values.
                Spike_amplitudes()
                Spike_rise_times(y_values) 
                Half_widths(y_values) 
                
                Reject_spikes(starts_final) # Rejects based on amplitude. Creates dictionary entries for included only spikes and their parameters..
                
                print("Generating spikes stats...")
                spike_IDs, ISIs, spikes_per_stim, av_ISI_per_stim, sd_ISI_per_stim, bursts = Spike_stats(stim_starts, spike_parameters_["starts_included"])

                #  ========  UNIT SCALING, SAMPLES -> SECONDS: ================================= 
                #Need a better place/ way to do this
                def scale_stuff():
                    pass
                print("Scaling data...")
                starts_secs = [x/fs for x in starts] # used for plotting detection dots only
                starts_scaled = [x+trim_offset for x in starts_secs]  # used for plotting detection dots only
                starts2_secs = [x/fs for x in starts2] # used for plotting detection dots only
                starts2_scaled = [x+trim_offset for x in starts2_secs] # used for plotting detection dots only
                
                starts_final_secs = [x/fs for x in spike_parameters_["starts_included"]]
                starts_final_scaled = [x+trim_offset for x in starts_final_secs]

                
                bl_x_secs = [x/fs for x in bl_x]
                bl_x_scaled = [x+trim_offset for x in bl_x_secs]
                
                local_max_t_secs = [x/fs for x in spike_parameters_["local_max_t"]]
                local_max_t = [x + trim_offset for x in local_max_t_secs]
                local_min_t_secs = [x/fs for x in spike_parameters_["local_min_t"]]
                local_min_t = [x + trim_offset for x in local_min_t_secs]
                
                local_min_t_included_secs = [x/fs for x in spike_parameters_["local_min_t_included"]]
                local_min_t_included = [x/fs for x in local_min_t_included_secs]

                
                local_min_t_secs = [x/fs for x in spike_parameters_["local_min_t"]] #for bottom dots.
                spike_parameters_["local_min_t_display"] = [x + trim_offset for x in local_min_t_secs]
                
                spike_start_times_secs = [x/fs for x in spike_parameters_["spike_start_times"]] #For start dots.
                spike_start_times = [x + trim_offset for x in spike_start_times_secs]
                
                half_width_lhe_secs = [x/fs for x in spike_parameters_["half_width_LHEs"]]
                half_width_lhe = [x + trim_offset for x in half_width_lhe_secs]
                half_width_rhe_secs = [x/fs for x in spike_parameters_["half_width_RHEs"]]
                half_width_rhe = [x + trim_offset for x in half_width_rhe_secs]

                # ==========  WRITE EXCEL          ==========================================
                info = "Slope: " + str(Slope) +"mV/sample; Threshold: "+str(spike_threshold)+"mV; Amp: "+str(Amplitude)+ "mV; filter:"+filter_type+ " "+ str(lowcut) + ", " + str(highcut)+ "Hz; Trial length : "+ str(sample_to_sec(len(y_values))) + "s; EPSP Highcut: " + str(epsp_highcut) + "Hz"
                #info2 is stim artefact detection stuff.
                info2 = "; Stim blanking: "+str(stim_blanking)+ "ms; Stim peak: " + str(stim_peak) + "mV; Slope dt: "+ str(Slope_dt) + "samples; Stim slope thresh: " + str(stim_slope_thresh) + "mV; stim refractory: " + str(stim_refractory) + "ms; Max spike freq:" + str(max_spike_freq) + "Hz; Min. burst freq" + str(min_burst_freq) +"Hz"
                info = info + info2
                write_trial_worksheet(starts_scaled, stim_starts_scaled, ISIs, spike_IDs, spike_threshold, info, bursts)
                
                # ==========  WRITE TO CUMULATIVE STATS ARRAY  for each trial ==========================================
                def write_summary_stats_array(): #marker
                    pass
                sum_["durations"].append((sample_to_sec(len(y_values))))
                sum_["M_n_spikes"].append(len(spike_parameters_["starts_included"]))
                if len(ISIs) > 1:
                    sum_["M_av_isi"].append(stats.mean(ISIs)*1000) 
                    sum_["M_sd_isi"].append(stats.stdev(ISIs)*1000)
                else:
                    sum_["M_av_isi"].append(0)
                    sum_["M_sd_isi"].append(0)
                sum_["M_n_bursts"].append(spike_stats_["n_bursts"])
                sum_["M_av_burst_f"].append(spike_stats_["mean_burst_freq"]) 
                sum_["M_sd_burst_f"].append(spike_stats_["sd_burst_spike_freq"])
                if len(spike_stats_["spikes_per_burst"]) > 1:
                    sum_["M_av_spike_per_burst"].append(stats.mean(spike_stats_["spikes_per_burst"]))
                    sum_["M_sd_spike_per_burst"].append(stats.stdev(spike_stats_["spikes_per_burst"]))
                else:
                    sum_["M_av_spike_per_burst"].append(0)
                    sum_["M_sd_spike_per_burst"].append(0)
                sum_["M_max_spike_per_burst"].append(max(spike_stats_["spikes_per_burst"]))
                sum_["M_av_IBI_n_spikes"].append(spike_stats_["av_ibi_n_spikes"])
                sum_["M_av_IBI_ISI"].append(spike_stats_["mean_IBI_ISI"]*1000)
                sum_["M_av_IBI_dur"].append(spike_stats_["av_IBI_dur"]*1000)
                sum_["M_%_spikes_in_bursts"].append(sum(spike_stats_["spikes_per_burst"])/(len(spike_parameters_["starts_included"]))*100)
                # ==========  GRAPHING          ==========================================
                
                #Create t-axis values 
                t_values = np.arange(t_startpoint, t_endpoint-1) #x axis units are samples -_____-
                if t_display_units == "milliseconds":
                    t_values_graph = (t_values/fs)*1000
                elif t_display_units == "seconds":
                    t_values_graph = (t_values/fs)
                else:
                    print("Invalid entry for t_display_units. Defaulting to units of seconds.")
                    t_values_graph = (t_values/fs)
                #========== FILTERED SIG GRAPH  ===============
                def graph_filtered_sig():
                    print("Graphing...")
                    plt.style.use('seaborn-whitegrid')
                    plt.figure(figsize=graph_size)
                    title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number)   + " S" + str(Slope) + ", T" + str(spike_threshold) +", A" + str(Amplitude) + ", dt" + str(Slope_dt) +  " filt.png"
                    #title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number)  + " thresh" + str(spike_threshold) + " filt.png"
                    plt.title(title)
                    plt.xlabel(t_display_units)
                        #Plot neural data
                    try:
                        plt.plot(t_values_graph, filtered_signal, linewidth=1) # # filtered signal
                        #plt.plot(t_values_graph, raw_unblanked_sig, linewidth=0.5, c="Red") # raw signal
                        #plt.plot(t_values_graph, filtered_signal, 'go')
                    except:
                        plt.plot(t_values_graph, filtered_signal[:-1], linewidth=1) # filtered signal
                        #plt.plot(t_values_graph, raw_unblanked_sig[:-1], linewidth=0.5, c="Red") # raw signal
                        #plt.plot(t_values_graph, filtered_signal[:-1], 'go')
                    plt.scatter(epsp_starts_scaled, np.full(len(epsp_starts_scaled), 0.02), marker="x", s=120, c="Yellow") #EPSP starts
                    plt.scatter(epsp_stops_scaled, np.full(len(epsp_stops_scaled), 0.02), marker="x", s=120, c="Green") #EPSP stops
                    plt.scatter(starts_scaled, np.full(len(starts), spike_threshold), marker="D", c="Black") #Plot spike threshold DETECTION dots
                    plt.scatter(starts2_scaled, np.full(len(starts2), spike_threshold-0.05), marker="D", c="Blue") # spike slope DETECTION dots
                    if stim_n_spikes > 0:
                        plt.scatter(stim_starts_scaled, np.full(stim_n_spikes, 0.1), marker="D", c="Orange") #Stim starts
                    plt.scatter(stim_stops_scaled, np.full(len(stim_stops_scaled), 0.1), marker="x", c="Orange") #Stim stops
                    #plt.yticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_x_interval, graph_x_interval))
                    if graph_x_interval != "auto":
                        plt.xticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_y_interval, graph_y_interval))
                    #for xc in stim_starts_scaled:
                    #    plt.axvline(x=xc, c="Yellow")
                    #plt.tight_layout()
                    #plt.show
                    plt.savefig(directoryy + title)
                    plt.clf()
                    plt.close('all')
                graph_filtered_sig()
                
                #==========   RAW SIG GRAPH   ===============
                def graph_raw_sig():    
                    plt.figure(figsize=graph_size)
                    #title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number) + " thresh" + str(spike_threshold) + " orig+filt.png"
                    title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number)  + " S" + str(Slope) + ", T" + str(spike_threshold) +", A" + str(Amplitude) + ", dt" + str(Slope_dt) + " orig.png"
                    plt.title(title)
                    plt.xlabel(t_display_units)
                    #Plot neural data
                    try:
                        #plt.plot(t_values_graph, filtered_signal, linewidth=1) # # filtered signal
                        plt.plot(t_values_graph, y_values, linewidth=0.5, c="Red", label="Raw signal") # raw signal
                        #plt.plot(t_values_graph, filtered_signal, 'go')
                    except:
                        #plt.plot(t_values_graph, filtered_signal[:-1], linewidth=1) # filtered signal
                        plt.plot(t_values_graph, y_values[:-1], linewidth=0.5, c="Red", label="Raw signal") # raw signal
                        #plt.plot(t_values_graph, filtered_signal[:-1], 'go')

                    plt.scatter(epsp_starts_scaled, np.full(len(epsp_starts_scaled), 0.02), marker="x", s=120, c="Yellow", label="EPSP start") #EPSP starts
                    plt.scatter(epsp_stops_scaled, np.full(len(epsp_stops_scaled), 0.02), marker="x", s=120, c="Green", label="EPSP stop") #EPSP stops
                    detect_dots_top_y = min(y_values) 
                    plt.scatter(starts_scaled, np.full(len(starts), detect_dots_top_y-0.001), marker=".", c="Red", label="Detected by Threshold", alpha=0.3) #Plot spike threshold DETECTION dots
                    plt.scatter(starts2_scaled, np.full(len(starts2), detect_dots_top_y-0.003), marker=".", c="Blue", label="Detected by Slope", alpha=0.3) # spike slope DETECTION dots
                    plt.scatter(starts_final_scaled, np.full(len(starts_final_scaled), detect_dots_top_y -0.005), marker="^", c="Green", label="Spike Accepted") 

                    # ======= Spike component plotting
                    #plt.scatter(bl_x_scaled, bl_y, marker="H", c="Yellow") #Plot baseline position(s)
                    plt.scatter(local_max_t, spike_parameters_["local_max_y"], marker="D", c="Green", alpha=0.3, label="Positive peak") #Plot local maximum
                    plt.scatter(local_min_t, spike_parameters_["local_min_y"], marker="D", c="Red", label="Spike bottom") #Plot local minimum
                    plt.scatter(spike_start_times, spike_parameters_["baselines_y"], marker="D", c="Yellow", label="Spike starts") #Plot spike starts
                    plt.scatter(half_width_lhe, spike_parameters_["half_width_LHEs_y"], marker="2", c="Black", label="Half width") #Plot half width stuff
                    plt.scatter(half_width_rhe, spike_parameters_["half_width_RHEs_y"], marker="2", c="Black") #Plot half width stuff
                    ####plt.scatter(local_min_t, spike_parameters_["half_width_y"], marker="X", c="Red") #Plot half width stuff
                    if stim_n_spikes > 0: #Plot dots for where stim artefact was.
                        #plt.scatter(stim_starts_scaled, np.full(stim_n_spikes, 0.025), marker="D", c="Orange") #Stim starts
                        #plt.scatter(stim_stops_scaled, np.full(len(stim_stops_scaled), 0.025), marker="x", c="Orange") #Stim stops
                        plt.axvline(x=stim_starts_scaled[0], c="Yellow", label="Stim artefact")
                        if stim_n_spikes > 1:
                            for xc in stim_starts_scaled[1:]:
                                plt.axvline(x=xc, c="Yellow")
                    if graph_x_interval != "auto":
                        plt.xticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_x_interval, graph_x_interval))

                    # ======= Spike info labelling
                    if spike_label_display == "Details":
                        for i in range(len(spike_parameters_["starts_included"])): #down deflection only
                            #plt.annotate(text, (x coord, y coord))
                            text_label = "\n\n\n\n\n   t:" + str(round(starts_final_scaled[i],3))
                            text_label = text_label + ("\n   A:" + str(round(spike_parameters_["amplitudes_included"][i],5)) + "\n   R:" + str(spike_parameters_["spike_rise_times"][i]) + "\n   H:" + str(spike_parameters_["half_widths"][i]))
                            #print("local_min_t_included",local_min_t_included[i])
                            plt.annotate(text_label, (sample_to_sec(spike_parameters_["local_min_t_included"][i]), spike_parameters_["local_min_y_included"][i]))
                    elif spike_label_display == "Number":
                        for i in range(len(spike_parameters_["starts_included"])): 
                            text_label = str(round(starts_final_scaled[i],3))
                            plt.annotate(text_label, (sample_to_sec(spike_parameters_["local_min_t_included"][i]), spike_parameters_["local_min_y_included"][i]))
                    elif spike_label_display == "None":
                        pass #no labelling :D

                    plt.legend(loc="upper left", fancybox="true", fontsize="x-large", frameon="true")
                    plt.tight_layout()
                    if float(graph_focus[0]) != 0.0 and float(graph_focus[1]) != 0.0:
                        print("Graph refocused")
                        plt.xlim(graph_focus)
                    #plt.show()
                    plt.savefig(directoryy + title)
                    plt.clf()
                    plt.close('all')
                graph_raw_sig()
                #----> End of trial loop
            # ---> Return to Experiment loop
            #Summary stats
            #Slice Master variables according to pre, stim, post indices.
            def calculate_exp_summary_stats():
                print("Calculating summary stats...")
                slices_ = []
                dict_sum_var_list = ["Sum_av_n_spikes", "Sum_sd_n_spikes", "Sum_av_av_ISI", "Sum_sd_av_ISI", "Sum_av_n_bursts", "Sum_sd_n_bursts",
                "Sum_av_av_burst_f", "Sum_sd_av_burst_f", "Sum_av_av_spikes_per_burst","Sum_sd_av_spikes_per_burst","Sum_av_av_IBI_n_spikes","Sum_sd_av_IBI_n_spikes",
                "Sum_av_av_IBI_ISI","Sum_sd_av_IBI_ISI","Sum_av_av_IBI_dur","Sum_sd_av_IBI_dur"]
                for i in dict_sum_var_list: 
                    sum_[i] = [] #declare new variables in sum_ dictionary
                if pre_n_trials > 0:                 
                    slices_.append(slice(0, pre_n_trials)) #pre trial indices
                    slices_.append(slice(pre_n_trials+1, pre_n_trials + post_n_trials+1))
                    #print("Slices",slices_)
                    for slicee in slices_:
                        sum_["Sum_av_n_spikes"].append(round(stats.mean(sum_["M_n_spikes"][slicee]), 1))
                        sum_["Sum_sd_n_spikes"].append(round(stats.stdev(sum_["M_n_spikes"][slicee]), 1))
                        sum_["Sum_av_av_ISI"].append(round(np.average(a=(sum_["M_av_isi"][slicee]), weights= sum_["M_n_spikes"][slicee]), 1))
                        sum_["Sum_sd_av_ISI"].append(round(weighted_std(sum_["M_av_isi"][slicee], sum_["M_n_spikes"][slicee]), 1))
                        sum_["Sum_av_n_bursts"].append(round(stats.mean(sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_sd_n_bursts"].append(round(stats.stdev(sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_av_av_burst_f"].append(round(np.average(a=(sum_["M_av_burst_f"][slicee]), weights= sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_sd_av_burst_f"].append(round(weighted_std(sum_["M_sd_burst_f"][slicee], sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_av_av_spikes_per_burst"].append(round(np.average(a=(sum_["M_av_spike_per_burst"][slicee]), weights= sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_sd_av_spikes_per_burst"].append(round(weighted_std(sum_["M_sd_spike_per_burst"][slicee], sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_av_av_IBI_n_spikes"].append(round(np.average(a=(sum_["M_av_IBI_n_spikes"][slicee]), weights= sum_["M_n_bursts"][slicee]), 2))
                        sum_["Sum_sd_av_IBI_n_spikes"].append(round(weighted_std(sum_["M_av_IBI_n_spikes"][slicee], sum_["M_n_bursts"][slicee]), 2))
                        sum_["Sum_av_av_IBI_ISI"].append(round(np.average(a=(sum_["M_av_IBI_ISI"][slicee]), weights= sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_sd_av_IBI_ISI"].append(round(weighted_std(sum_["M_av_IBI_ISI"][slicee], sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_av_av_IBI_dur"].append(round(np.average(a=(sum_["M_av_IBI_dur"][slicee]), weights= sum_["M_n_bursts"][slicee]), 1))
                        sum_["Sum_sd_av_IBI_dur"].append(round(weighted_std(sum_["M_av_IBI_dur"][slicee], sum_["M_n_bursts"][slicee]), 1))
                else: #If stim trial only:
                    #write list of three dashes to each sum dict entry.
                    for i in range (2):
                        for item in dict_sum_var_list: sum_[item].append("-")  
                            
            calculate_exp_summary_stats()
            write_summary_worksheet(info)
            workbook.close()
            gc.collect()

        # !!!!!!!!! Write filtered signal to a worksheet =============================:
        '''print("Writing filtered signal data to Excel spreadsheet...")
        worksheet = workbook.add_worksheet("Filtered signal")
        [worksheet.write(i, 0, filtered_signal[i]) for i in range(len(filtered_signal))]
        worksheet.write(0, 1, "sig blank starts")
        worksheet.write(0, 2, "sig blank stops")
        [worksheet.write(i+1, 1, stim_blank_starts[i]) for i in range(len(stim_blank_starts))]
        [worksheet.write(i+1, 2, stim_blank_stops[i]) for i in range(len(stim_blank_stops))]'''
    
        #title_number += 1
    # FINISHED RUNNING EXPERIMENT, now pause and ask user wot to do next ===================================================:
   
    end_time = time.time()
    time_elapsed = end_time - master_start_time
    print("Program time elapsed:",time_elapsed," seconds.")
    print("Current time:",time.asctime())
    '''try:
        os.system('say "program finished"')
    except:
        continue'''
    print("Analysis finished Â¯\_( Í¡â›â€¯ÍœÊ– Í¡â›)_/Â¯")
    
    def update_param(param, dtype):
        loop = True
        while loop == True:
            if dtype == "float":
                value = input("Enter new value for "+ param + ":>_ ")
                try: globals()[param], loop = float(value), False
                except: print("Sorry you did not enter correct value for", param, ", please try again...\n:>_")
            elif dtype == "float_list":
                value = input("Enter new values for "+ param + " as a space-separated list\n:>_ ")
                try:
                    value = value.split()
                    for i in range(len(value)): value[i] = float(value[i])
                    globals()[param], loop = value, False
                except: print("Sorry you did not enter correct list value for",param, ", please enter as follows e.g.: 0.01 0.04 -0.4 \n:>_")
            elif dtype == "int_list":
                value = input("Enter new integer values for "+ param + " as a space-separated list\n:>_ ")
                try:
                    value = value.split()
                    for i in range(len(value)): value[i] = int(value[i])
                    globals()[param], loop = value, False
                except: print("Sorry you did not enter correct list value for",param, ", please enter as follows e.g.: 9 17 23\n:>_")
            elif dtype == "int":
                value = input("Enter new integer value for "+ param + "\n:>_ ")
                try: globals()[param], loop = int(value), False
                except: print("Sorry you did not enter correct list value for",param, ", please enter as follows e.g.: 9\n:>_")
            elif dtype == "t_analyze_window":
                value = input("Enter two space-separated float values for "+ param + " or enter 'all' \n:>_ ")
                if value != "all":
                    try:
                        value = value.split()
                        for i in range(len(value)): value[i] = float(value[i])
                        globals()[param], loop = value, False
                    except: print("Sorry you did not enter correct list value for",param, ", please enter as follows e.g.: 1.0 6.0\n:>_")
                else: globals()[param], loop = value, False
        print("\n ######################################################")
        print( "###  Value of ",param," changed to", str(value), " ###")
        print("######################################################")

    # ============ User halt while loop  =============
    user_halt = True
    print("\n\n Welcome to spike detector thing")
    while user_halt == True:
        display_string1 = "*****  Enter your letter selection:  *****\n\n=====  Adjust spike parameters:  ===== \n S: Slope^ [mV/sample] = " + str(Slope)
        display_string2 = "\n T: Thresholds^ (iterable) [mV] = " + str(Thresholds) + " \n A: Amplitude [mV] =  " + str(Amplitude) + "\n R: rise time [ms] = " + str(Amplitude)
        display_string3 = "\n\n=====  Other parameters to adjust:  ===== \n W: Time window to analyze = "+str(t_analyze_window)+" \n N: Stim trial numbers = " + str(stim_trial_numbers)+ "\n E: Number of pre trials = " + str(pre_n_trials)+ "\n O: Number of post trials = " +str(post_n_trials)+ "\n G: Graph size = " +str(graph_size)+" \n X: graph x interval (s) = " +str(graph_x_interval)
        display_string4 = "\n F: graph focus (left, right edges) = " + str(graph_focus) + "\n\n =====  Runtime commands:  ===== \n 0: Re-graph \n 1: Re-run \n 2: Stop \n\n:>_"
        display_string_yep = display_string1 + display_string2 +  display_string3 + display_string4
        choice = input(display_string_yep)
        if choice == "S" or choice == "s": update_param("Slope", "float")
        elif choice == "T" or choice == "t": update_param("Thresholds", "float_list")
        elif choice == "A" or choice == "a": update_param("Amplitude", "float")
        elif choice == "R" or choice == "r": update_param("Rise_times", "float")
        elif choice == "W" or choice == "w": update_param("t_analyze_window", "t_analyze_window")
        elif choice == "N" or choice == "n": update_param("stim_trial_numbers", "int_list")
        elif choice == "E" or choice == "e": update_param("pre_n_trials", "int")
        elif choice == "O" or choice == "o": update_param("post_n_trials", "int")
        elif choice == "G" or choice == "g": update_param("graph_size", "int_list")
        elif choice == "X" or choice == "x": update_param("graph_x_interval", "float")
        elif choice == "F" or choice == "f":
            print("Graph focus param: enter 0 0 to graph whole of t_analyze_window")
            update_param("graph_focus", "float_list")
        elif choice == "0":
            graph_filtered_sig()
            graph_raw_sig()
            print(" === Graphs re-rendered ===")
        elif choice == "1":
            user_halt = False
        elif choice == "2":
            print(" #######   Program terminating. ð”¾ð• ð• dð•“ð•ªð•–.   #######")
            Run = "No"
            del data_float
            del y_values
            del t_values
            del t_values_graph
            del master_data
            del raw_unblanked_sig
            break