#Instructions: 
#    Place in a folder with a single .txt files generated by NACshow. 

#from scipy.io.wavfile import write
import math
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter, filtfilt
import xlsxwriter
import time
from scipy.stats import linregress
import statistics as stats
import gc
#from scipy.interpolate import UnivariateSpline

''' ============== ====================== ====================
    ==============    User parameters     ===================
    ============== ====================== ===================='''
    
fs = 20000 #Hz --> Must match the sampling frequency of the experiment. Program cannot interpret sampling rate from the data files.
graph_size = (120,12) # Dimensions of the output graph image file. Not sure what units these are, maybe inches?????????????

#stim_trial_numbers = [21, 87, 151, 203, 267, 331] #Enter the trial numbers that stim trains occur in (see NAC file)
stim_trial_numbers = [35, 119, 183, 243, 325, 387]  #Enter the trial numbers that stim trains occur in (see NAC file)
#stim_trial_numbers = [35]

pre_n_trials = 9
post_n_trials = 15
#pre_n_trials = 1
#post_n_trials = 1


n_channels = 2 #The number of recording channels, choose 1 or 2.
    
t_display_units = "seconds" #DON'T CHANGE Yet.

#t_analyze_window = (4.7, 4.8) #[secs] start & end time of processing each trial, as a list of two numbers. Write "all" to not trim data.
t_analyze_window = "all"

filter_type = "bandpass" #choose "highpass",  "lowpass", "bandpass" or "none"
    #highpass - no phase lag. bandpass causes phase lag.
lowcut = 300 #(Hz) #Ben and my data 5000.
highcut = 5000.0 #(Hz). #cox cox gunn: 3000. Ben and my data 5000.
filter_order = 2
graph_x_interval = "auto" #[seconds] or write "auto"
graph_y_interval = 0.1 
#graph_focus = [2,3] #!!!!!!!!!!! Time interval in seconds to graph, set to 0 to graph whole trial (not implemented yet)

# ====== Stim detection function parameters ==========
stim_peak = 0.1 #[mV] Positive threshold to trigger stim artefact detector.
stim_blanking = 1 #[ms], width of stim artefact to be blanked, around midpoint of stim artefact
slope_dt = 1 #[samples], the increment in samples over which to check stim slope
stim_slope_thresh = 0.2 #Rising edge slope of stim artefact must be greater than this. 6/2/21 woz 1

stim_refractory = 0 #[ms] Period between stim and start of EPSP.
blank_refractory = "yes" #!!!!!!"yes", "no", blanks the period between stim artefact and EPSP. Auto blanks between stim and start of EPSP (Doesn't do anything yet)
#epsp_padding = 100 #[ms], gap between EPSP analysis and stim artefact.
epsp_highcut = 500 #[Hz] cutoff freq of EPSP LPF. 500Hz was original value.
epsp_length = 21 #[ms] length of EPSP after stim refractory period.

# ====== Spiking stuff ==============================
#spike_thresholds = [-0.025, -0.035, -0.045]  #[mV] list of numbers. Negative = downward deflecting spikes. Positive = upward deflecting spikes.
spike_thresholds = [-0.025, -0.035, -0.045]
max_spike_freq = 250 #[Hz], reject spikes that occurs 1/f time after previous spike.
min_burst_freq = 20 #[Hz], the minimum frequency of consecutive spiking events for them to be considered part of the same burst.
burst_min_n = 2 #!!!!!!!Minimum number of spikes in a burst. Doesn't do anything yet:- set at 2.
#poly_deg = 30 #under development, it gonna be cool, use splines
''' ========= END USER PARAMETERS ====================================================== '''

''' ============== ====================== ====================
    ==============    Start of Program    ===================
    ============== ====================== ===================='''

# IMPORT DATA ===============
start_time = time.time()
master_start_time = time.time()

#Declare dictionaries:
spike_stats_ = {}
    
#load file data into one list of floats, and generate indices of gaps between trial data
directory = "/"
dir_list = os.listdir(".")
print("Text files in this directory:")
for file_name in dir_list:
    if ('.txt' in file_name[-4:]):
        print(file_name)
        file = file_name
print("====================")
print("File to be processed:")
print(file)
print("Processing file, please wait.... ")

start_time = time.time()
f = open(file, 'r')
data = np.empty(1)
data = [x.strip().split('\t') for x in f]
f.close() 
print("time doing strip split",time.time()-start_time)
start_time = time.time()
data_0 = [item[0] for item in data] #gets rid of those annoying quotations marks that each data item has when imported.
data_float = [float(x) if x!='' else 0 for x in data_0] #if gap in data, replace with a 0.
print("time converting to floats",time.time()-start_time)
del data_0
# ============================
print("file data beginning snippet",data_float[0:10])
gap_list = [(index) for index, element in enumerate(data) if element == ['']] #generates list of indices for starts of trials 
gap_list = ([-1] + gap_list)   
n_trials = len(gap_list)-1 
n_trials = len(gap_list) 
del data
#Trial number x starts at gap_list[x]
end_time = time.time()
time_elapsed = end_time - start_time
print("Finished initial loading of data, total time elapsed (seconds): ",time_elapsed)

'''
 ====================================================
 ======              FILTER FUNCTIONS               =======:
 ===================================================='''
def high_pass_filter(data, fc, fs, order):
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='highpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = lfilter(b, a, data)
    return filtered_data

'''def low_pass_filter(data, fc, fs, order): #has phase shift
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='lowpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = lfilter(b, a, data)
    return filtered_data'''

def low_pass_filter2(data, fc, fs, order): #no phase shift
    nyq = 0.5 * fs
    cutoff = fc/nyq
    #scipy.signal.butter(9, Wn, btype='low', analog=False, output='ba', fs=None) #returns two items, 
    b, a = butter(order, cutoff, btype='lowpass', analog=False, output='ba', fs=None) #returns critical bounds to be used for actual filter , 
    filtered_data = filtfilt(b, a, data)
    return filtered_data

def butter_bandpass(lowcut, highcut, fs, order=filter_order):
    nyq = 0.5 * fs
    low = lowcut / nyq
    #print("low",low)
    high = highcut / nyq
    #print("high",high)
    b, a = butter(order, [low, high], btype='band')
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=filter_order): #called if bandpass
    b, a = butter_bandpass(lowcut, highcut, fs, order=filter_order)
    y = lfilter(b, a, data)
    return y

''' ============================================================
 ====== Spike detection and signal processing functions  =======:
 =============================================================='''

def sample_to_sec(samples): #converts a sample number to absolute time in seconds
    if t_analyze_window == "all":
        secs = samples/fs
    else:
        secs = samples/fs + t_analyze_window[0]
    return secs 

def generate_starts_stops(sig,spike_threshold): #filtered_signal = array, spike_threshold = voltage value
   #spiketrain_overthresh_bool declare size beforehand.
   if spike_threshold > 0:
       deflection = "up"
   else:
       deflection = "down"
   print("Spike deflection = ", deflection, " threshold ", spike_threshold)
   start_time = time.time()

   starts = [0]
   stops = []   
   spiketrain_overthresh_bool = []
   i = 0
   siglength = len(sig)
   #print("siglength from generate starts stops ",siglength)
   spiketrain_overthresh_bool = [0] * siglength    #spiketrain_overthresh_bool declare size beforehand.
   print("Spike detector running.....")
   for i in range(siglength):
      progress_indicator = divmod(i, fs)
      if progress_indicator[1] == 0:
          print(i/fs, "seconds processed")
          
      if deflection == "up":
          #Upward spike deflection
          #print("debug sig, spike_threshold",sig[i],",",spike_threshold)
          if float(sig[i]) >= float(spike_threshold):  # 
             #spiketrain_overthresh_bool = np.append(spiketrain_overthresh_bool, "1")
             spiketrain_overthresh_bool[i] = 1
             #print("      threshold sample")
          else:
             #spiketrain_overthresh_bool = np.append(spiketrain_overthresh_bool, "0")
             spiketrain_overthresh_bool[i] = 0
      elif deflection == "down":     
          #Downward spike deflection:
          if float(sig[i]) <= float(spike_threshold):  
             spiketrain_overthresh_bool[i] = 1
             #print("  threshold sample, sig = ",float(sig[i]), "spike_threshold ", float(spike_threshold))
          else:
             spiketrain_overthresh_bool[i] = 0
             #print(" under threshold sample, sig = ",float(sig[i]), "spike_threshold ", float(spike_threshold))
  
      if i > 0:
          if int(spiketrain_overthresh_bool[i]) > int(spiketrain_overthresh_bool[i-1]) and i > (starts[-1] + fs/max_spike_freq): # !!!!!!!! Reject spike if too soon after previous.

             starts.append(i)
             #print("*Spike Detected ")
             #is burst?
          if spiketrain_overthresh_bool[i-1] > spiketrain_overthresh_bool[i]:
             stops.append(i)
             
   #Generate spike widths...
   starts_stops_length = int(len(starts))
   spike_widths = []
   for k in range(0,starts_stops_length-1):
       spike_widths = np.append(spike_widths, stops[k] - starts[k])
   n_spikes = len(starts)
   print("number of spikes detected",n_spikes)
   end_time = time.time()
   time_elapsed = end_time - start_time
   print("time elapsed detecting regular spikes",time_elapsed)
   #remove first start of 0:
   #starts.pop(0)
   return(starts, stops, spike_widths, n_spikes) #units of starts, stops, spike_widths = SAMPLES

def stim_train_detect(sig, stim_cut):     #returns list of spike starts and stops, in samples.
    start_time = time.time()
    stim_starts = []
    stim_stops = []
    i = 0
    siglength = len(sig)
    #print("stim_train_detect: sig length",siglength)
    #print("sig shape",np.shape(sig))
    #print("sig sample",sig[0:10])
    stimtrain_overthresh_bool = [0] * siglength
    is_included = "no"
    for i in range(siglength): # i = [samples]
          #progress_indicator = divmod(i, fs)
          #if progress_indicator[1] == 0:
              #print("Sample number: ",i)
           
              #Upward spike deflection:
          #print("type stim_cut ",type(stim_cut))
          if float(sig[i]) >= float(stim_cut):  # upward deflection
               stimtrain_overthresh_bool[i] = 1
               #print("      threshold sample")
          else:
               stimtrain_overthresh_bool[i] = 0
          if i > 0:
              if (stimtrain_overthresh_bool[i]) > (stimtrain_overthresh_bool[i-1]): 
                 #check slope before adding start
                 slope, intercept, r, p, se = linregress([i-slope_dt, i], [sig[i-slope_dt],sig[i]])
                 #print("candidate starts slope", slope, "at ", sample_to_sec(i), "seconds")
                 if slope >= stim_slope_thresh:
                     stim_starts.append(i)
                     print("*Stim artefact rising edge detected, slope ",slope, "at ",sample_to_sec(i))
                     is_included = "yes"
                 else:
                     is_included = "no"
              if stimtrain_overthresh_bool[i-1] > stimtrain_overthresh_bool[i]:
                 if is_included == "yes":
                     #stim_stops = np.append(stim_stops, i)\
                     #slope, intercept, r, p, se = linregress([i-slope_dt, i], [sig[i-slope_dt],sig[i]])
                     #print("stim stop at ", sample_to_sec(i))
                     #if slope <= -stim_slope_thresh:
                     stim_stops.append(i)
    starts_stops_length = int(len(stim_starts))
    stim_spike_widths = []
    for k in range(0,starts_stops_length-1):
        stim_spike_widths = np.append(stim_spike_widths, stim_stops[k] - stim_starts[k])
    stim_n_spikes = len(stim_starts)
    print("number of stim spikes detected",stim_n_spikes, ", at locations", stim_starts)
    end_time = time.time()
    time_elapsed = end_time - start_time
    print("time elapsed detecting stim spikes",time_elapsed)
    print("stim starts",stim_starts)
    print("stim stops",stim_stops)
    return(stim_starts, stim_stops, stim_spike_widths, stim_n_spikes)     

def remove_stim_artefacts(stim_starts, stim_stops, sig_data):
    #write zeros between stim start and stop
    stim_midpoints = []
    for i in range(len(stim_starts)):
        stim_midpoints.append((stim_starts[i] + stim_stops[i])/2)
    #stim_midpoints = [(i + j)/2 for i in stim_starts for j in stim_stops]
    print("stim_midpoints",stim_midpoints)
    if blank_refractory == "no":
        sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
        sig_blank_stops = [(i + (stim_blanking/2000)*fs) for i in stim_midpoints]
        for i, j in zip(sig_blank_starts, sig_blank_stops):
            sig_data[int(i):int(j)] = 0
    elif blank_refractory == "yes":
        sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
        sig_blank_stops = [(i + (stim_blanking/2000)*fs + (stim_refractory/1000)*fs) for i in stim_midpoints]
        for i, j in zip(sig_blank_starts, sig_blank_stops):
            sig_data[int(i):int(j)] = 0 #Writes zero to the signal data values within blanking intervals.
    stim_midpoints_scaled = [sample_to_sec(x) for x in stim_midpoints]
    print("stim artefacts removed at",stim_midpoints_scaled, " seconds")
    return sig_data, sig_blank_starts, sig_blank_stops, stim_midpoints
        

def remove_epsps(stim_starts, stim_stops, signal):
    #takes whole signal and subtracts low freq waveform proceeding stim artefact.
    # returns signal, list of starts, list of stops of epsp removal section
    #!!!!! TO DO: fix glitch at end of EPSP. Possible solution write a straight line with two endpoints instead of 0
    epsp_starts = []
    epsp_stops = []
    #stim_midpoints = [] #New
    for i in range(len(stim_starts)):
        # print("remove epsp i",i)
        epsp_t_start = int(stim_stops[i] + (stim_refractory/1000)*fs) #[samples]
        epsp_t_stop = int(epsp_t_start + (epsp_length/1000)*fs) #[samples]
        epsp_starts.append(epsp_t_start)
        epsp_stops.append(epsp_t_stop)
        print("epsp_t_start (secs)",sample_to_sec(epsp_t_start))
        print("epsp_t_stop (secs)",sample_to_sec(epsp_t_stop))
        sig_epsp_waveform = low_pass_filter2(signal[epsp_t_start:epsp_t_stop], epsp_highcut, fs, order=2)
        #subtract above from whole signal between indices.
        signal[epsp_t_start:epsp_t_stop] = signal[epsp_t_start:epsp_t_stop] - sig_epsp_waveform
        '''spl = UnivariateSpline(x, y, w=~w)
        #Try univariate spline 5/26/21
        #https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline
        
        #Polyfit didn't work...
        #poly_coeffs = np.polyfit(range(epsp_t_start, epsp_t_stop), sig_epsp_waveform, poly_deg, rcond=None, full=False, w=None, cov=False)
        #print("poly coefficients",poly_coeffs)
        #epsp_poly_Waveform = np.poly1d(poly_coeffs)
        
        new_x = range(epsp_t_start, epsp_t_stop)
        new_y = spl(new_x)
        
        #Plot poly
        plt.figure(figsize=graph_size)
        title = "poly"
        plt.title(title)
        #plt.xlabel(t_display_units)
        #Plot neural data
        plt.plot(new_x, new_y, linewidth=1) # poly
        #print("len new_x",len(new_x))
        #print("len new_x",len(new_x))
        plt.scatter(new_x, sig_epsp_waveform, marker="D", c="Orange") # orig sig
        plt.show()
        plt.close()'''
        '''
        plt.figure(figsize=graph_size)
        title = "EPSP"
        plt.title(title)
        plt.xlabel(t_display_units)
        #Plot neural data
        plt.plot(range(epsp_t_start,epsp_t_stop), sig_epsp_waveform, linewidth=1) # # filtered signal'''
        
        '''
        #Refractory blanking if choice = auto:
        stim_midpoints.append((stim_starts[i] + stim_stops[i])/2)
        if blank_refractory == "auto":
            sig_blank_starts = [(i - (stim_blanking/2000)*fs) for i in stim_midpoints]
            sig_blank_stops = [(i + (stim_blanking/2000)*fs + (stim_refractory/1000)*fs) for i in stim_midpoints]
            for i, j in zip(sig_blank_starts, sig_blank_stops):
                sig_data[int(i):int(j)] = 0
        '''
    return signal, epsp_starts, epsp_stops

def stim_period(stim_mids): #Returns the mean period between stim spikes
    inter_stim_intervals = [0] * (len(stim_mids)-1)
    for i in range(len(stim_mids)-1):
        inter_stim_intervals[i] = stim_mids[i+1] - stim_mids[i]
    print("inter_stim_intervals",inter_stim_intervals)
    stim_period_ = sum(inter_stim_intervals)/len(inter_stim_intervals)
    return stim_period_

def spike_stats(stim_mids, starts): #returns spike_id (which stim spike the spike proceeds), and isi_list
    #Generate spike ID's...:
    #append stim_mids with a virtual extra stim to categorize "post" spikes
    stim_mids2=stim_mids
    if len(stim_mids) > 1: #If there's more than one stim. If there isn't, it's probably a baseline trial ;)
        stim_mids2.append(stim_mids[-1]+(stim_mids[-1]-stim_mids[-2])) #adds an extra category for post stim train spikes, which start one inter-stim interval after last spike.
    #if len(stim_mids) > 0: #Occasionally stim artefact not detected or there is none.
        spike_ids = np.digitize(starts, stim_mids2, False) #Spike ID 0 means pre-stim spike. Assigns IDs to spikes based on which stim pulse they proceed                    
    else:
        spike_ids = np.zeros(len(starts), dtype=int).tolist()
    print("Spike IDs",spike_ids)
    #ISI Calculations: each spike except the first has a corresponding ISI.
    isi_list = [0] * (len(starts))
    for i in range(len(isi_list)-1):
        isi_list[i+1] = (starts[i+1] - starts[i])/fs
    spikes_per_stim = [0] * (len(stim_mids2)+1)
    for i in range(0,int(max(spike_ids)+1)):
        j = i
        for ID in spike_ids:
            if ID == j:
                spikes_per_stim[i] += 1    
    
    if len(spike_ids) == 0: #If no spikes were detected, need spike_ids to be not empty.
        spike_ids = 0    

    #Av. ISI per stim calculation:
    #for each spike ID, make list of ISIs and find the mean and sd of the list
    av_isi_per_stim = [] #len n_stim_spikes+2 (inc pre und post)
    sd_isi_per_stim = []
    isi_indices = []
    #isi_per_stim = []
    #print("######### Length stim mids",len(stim_mids))
    #print("#########  stim mids", stim_mids)

    if len(stim_mids) > 0:
        for j in range(len(stim_mids)+1):
            isi_per_stim = []
            isi_indices = []
            for i in range(len(spike_ids)):
                if spike_ids[i] == j:
                    isi_indices.append(i)
            [isi_per_stim.append(isi_list[k]) for k in isi_indices if isi_list[k] != 0]
            try:
                #print("Debug, ISIs per stim #", j, " ",isi_per_stim)
                av_isi_per_stim.append(stats.mean(isi_per_stim))
            except: #If there are no ISIs in this bin?
                av_isi_per_stim.append(0)
            try:
                sd_isi_per_stim.append(stats.stdev(isi_per_stim))
            except: #If there are no ISIs in this bin?
                sd_isi_per_stim.append(0)
    else:
        sd_isi_per_stim = stats.stdev(isi_per_stim)
        av_isi_per_stim = stats.mean(isi_per_stim)
    #print("av_isi_per_stim",av_isi_per_stim)
    #print("sd_isi_per_stim",sd_isi_per_stim)
    spike_stats_["sd_isi_per_stim"] = sd_isi_per_stim
    spike_stats_["av_isi_per_stim"] = av_isi_per_stim

    '''========== Spike bins: ============='''
    start_t_bins = np.arange(0, math.ceil(sample_to_sec(max(starts)))+1)
    #print("spike t bins",start_t_bins)
    starts_sec = [sample_to_sec(starts[i]) for i in range(len(starts))]
    starts_bin_counts = np.digitize(starts_sec, start_t_bins, True) 
    #print("starts_bin_counts",starts_bin_counts)
    starts_per_bin = [0] * (max(starts_bin_counts)+1)
    for i in range(max(starts_bin_counts)+1):
        for ID in starts_bin_counts:
            if ID == i:
                starts_per_bin[i] += 1    
    #print("starts_per_bin",starts_per_bin)
    starts_per_bin[0] -= 1 #Adjust first starts per bin bcoz it one too many
    spike_stats_["start_t_bins"] = start_t_bins
    spike_stats_["starts_per_bin"] = starts_per_bin

    #   ISIs per bin:  ==============
    isi_per_bin = []
    ticker = 0
    for i in range(len(starts_per_bin)): #other method: use bin counts as index iterator.
        isi_start_index = ticker
        isi_end_index = ticker + starts_per_bin[i]
        #print("DEBUG: isi_start_index", isi_start_index, "; isi_end_index", isi_end_index, "; ticker", ticker)
        #print("DEBUG: starts per bin", starts_per_bin[i])
        try:        
            av_isi = stats.mean(isi_list[isi_start_index:isi_end_index])
        except:
            av_isi = 0 #If there's no spikes in this bin
        ticker += starts_per_bin[i]
        isi_per_bin.append(av_isi)
    #print("ISI PER BIN",isi_per_bin)
    spike_stats_["isi_per_bin"] = isi_per_bin

    '''========== Bursting analysis: ============='''
    isi_list_arr = np.array(isi_list)
    is_burst = isi_list_arr <= (1/min_burst_freq) #initial identification of which spikes are part of a burst, excluding the first spike of a burst.
    burst_ID = [0] * len(is_burst)
    #print("is_burst",is_burst)
    burst_number = 0
    #Len(n_spikes) = len(spike_ids) = len(burst_ID).
    #Assign burst IDs to each spike (0 means es no part of burst)
    for i in range(1,len(is_burst)-1):
        if is_burst[i] == 1 and is_burst[i-1] == 0: #Start of a burst
            burst_number += 1 
            burst_ID[i-1] = burst_number
            burst_ID[i] = burst_number
        if is_burst[i] == 1: #middle of a burst? 
            burst_ID[i] = burst_number
        if is_burst[i] == 1 and is_burst[i+1] == 0: #End of burst
            #print("END OF BURST")
            burst_ID[i] = burst_number
        #print("burst_number", burst_number)
    #Handling last spike:
    if is_burst[-1] == 1:
        #print("last thing is 1")
        if is_burst[-2] == 0:
            #print("second to last thing is 0")
            burst_ID[-1] = burst_number 
            burst_ID[-2] = burst_number
        elif is_burst[-2] == 1:
            burst_ID[-1] = burst_number
    #print("is burst",is_burst)
    #print("burst_ID",burst_ID)
    n_bursts = burst_number #nice
    #print("n bursts",n_bursts)
    spikes_per_burst  = []
    for j in range(1,n_bursts+1):  #If first number in range is 0, then first number in list will be the number of zeros in burst_ID (i.e. number of spikes that are not part of a burst)
        spikes_per_burst.append(burst_ID.count(j))
    #Burst durations: time of last spike - time of first spike (= sum of ISIs except the first)
    burst_durations = []
    for i in range(1,n_bursts+1):#i: for each burst, burst index
        burst_index = []
        for j in range(len(burst_ID)): #j: scan index for entire burst_ID list. this method seems inefficient...
            if burst_ID[j] == i:
                burst_index.append(j) #creates list of indices of where burst is occurring.
        burst_start_index = burst_index[0]
        burst_end_index = burst_index[-1]
        #print("burst_start_index and value",burst_start_index, starts[burst_start_index])
        #print("burst_end_index and value",burst_end_index, starts[burst_end_index])
        burst_durations.append(sample_to_sec(starts[burst_end_index] - starts[burst_start_index])) #[seconds]        
    
    #interburst spikes:
    IBI_start = 0
    if n_bursts > 1:
        IBI_dur = [] #returned list
        IBI_n_spikes = [] #returned list
        IBI_av_ISIs = [] #returned list
        #IBI_sd_ISIs = []
        IBI_ISIs = []
        for j in range(len(burst_ID)-1): #writew IBI_n_spikes and IBI_av_ISIs for each burst ID
            IBI_av_ISI = []
            
            #lookback to check if IBI start
            if burst_ID[j] == 0 and burst_ID[j-1] != 0 and j != 0: # start new interburst when previous spike was part of a burst
                #print("start new IBI")
                IBI_start = j-1 
            elif burst_ID[j] != burst_ID[j-1]: #if one burst proceeds the previous with no IBI spikes......
                IBI_start = j-1
                
            #look forward to see if it is IBI end
            if burst_ID[j] == 0 and burst_ID[j+1] != 0: #end of IBI reached, do some calculations on it
                IBI_end = j+1 #index
                #print("IBI:",IBI_start, IBI_enccd)
                #Calculate interburst interval in seconds:
                IBI_t = sample_to_sec(starts[IBI_end] - starts[IBI_start])
                IBI_dur.append(IBI_t)
                    
                IBI_n_spikes.append(IBI_end - IBI_start - 1)
                #IBI ISI start at IBI_start+1 and end at IBI_end - 1
                #IBI_av_ISI = IBI_t / (IBI_end - IBI_start - 1) #<-- old way
                if IBI_end - IBI_start - 1 > 0: 
                    for i in range(IBI_start+1,IBI_end+1):
                        IBI_av_ISI.append(isi_list[i])
                        IBI_ISIs.append(isi_list[i])
                        #print("IBI ISI",isi_list[i])
                    #IBI_sd_ISI = stats.stdev(IBI_av_ISI)
                    IBI_av_ISI = stats.mean(IBI_av_ISI)
                    IBI_av_ISIs.append(IBI_av_ISI)
                    #IBI_sd_ISIs.append(IBI_sd_ISI)
                else:
                    IBI_av_ISIs.append(0)
                    #IBI_sd_ISIs.append(0)
            if burst_ID[j] != burst_ID[j+1] and burst_ID[j] != 0 and burst_ID[j+1] != 0: #one burst follows immediately after the next.
                    IBI_start = j-1
                    IBI_end = j 
                    IBI_t = sample_to_sec(starts[IBI_end] - starts[IBI_start])
                    IBI_dur.append(IBI_t)
                    IBI_n_spikes.append(0)
                    IBI_av_ISIs.append("")
                    #IBI_sd_ISIs.append("")

    else:
        IBI_dur = [0] #returned list
        IBI_n_spikes = [0] #returned list
        IBI_av_ISIs = [""] #returned list
        #IBI_sd_ISIs = [0] #returned list
    try:
        av_ibi_n_spikes = stats.mean(IBI_n_spikes[1:]) #First row of IBI stats is ignored - becoz it meaningless.
    except:
        av_ibi_n_spikes = 0
    #print("IBI DUR", IBI_dur)
    #print("IBI DUR trimmed", IBI_dur[1:])
    try:
        av_IBI_dur = stats.mean(IBI_dur[1:])
    except:
        av_IBI_dur = 0
    try:
        mean_IBI_ISI = stats.mean(IBI_ISIs)
    except:
        mean_IBI_ISI = 0
    spike_stats_["av_ibi_n_spikes"] = av_ibi_n_spikes
    spike_stats_["av_IBI_dur"] = av_IBI_dur
    spike_stats_["mean_IBI_ISI"] = mean_IBI_ISI
    #end interburst stuff        
    spikes_per_burst_ = [hahaha -1 for hahaha in spikes_per_burst]
    av_burst_spike_freq = np.divide(spikes_per_burst_, burst_durations) #[Hz]
    if av_burst_spike_freq.size > 0:
        mean_burst_freq = sum(av_burst_spike_freq)/len(av_burst_spike_freq)
    else:
        mean_burst_freq = 0
    if av_burst_spike_freq.size > 1:
        sd_burst_spike_freq = stats.stdev(av_burst_spike_freq)
    else:
        sd_burst_spike_freq = 0
    #print("mean_burst_freq",mean_burst_freq)
    #print("n bursts",n_bursts)
    #print("n spikes per burst",spikes_per_burst)
    #print("burst durations",burst_durations)
    #print("av_av_burst_spike_freq",av_burst_spike_freq)    #lists with length 
    #Write spike_stats_ entries with data:
    spike_stats_["spike_ids"] = spike_ids #len n_spikes
    spike_stats_["isi_list"] = isi_list #len n_spikes
    spike_stats_["spikes_per_stim"] = spikes_per_stim #len n_stim_spikes + 2 (pre & post)
    spike_stats_["av_isi_per_stim"] = av_isi_per_stim #len 1 for pre or post, or n_stim_spikes for stim trial.
    spike_stats_["sd_isi_per_stim"] = sd_isi_per_stim #len 1
    #Burst stats:
    spike_stats_["burst_ID"] = burst_ID #len n_spikes
    spike_stats_["mean_burst_freq"] = mean_burst_freq #len 1
    spike_stats_["n_bursts"] = n_bursts #len 1
    spike_stats_["spikes_per_burst"] = spikes_per_burst #len n_bursts
    spike_stats_["burst_durations"] = burst_durations #len n_bursts. [samples]
    spike_stats_["av_burst_spike_freq"] = av_burst_spike_freq #len n_bursts
    spike_stats_["sd_burst_spike_freq"] = sd_burst_spike_freq #len n_bursts
    spike_stats_["IBI_n_spikes"] = IBI_n_spikes #len n_IBIs
    spike_stats_["IBI_av_ISIs"] = IBI_av_ISIs #len n_IBIs
    #spike_stats_["IBI_sd_ISIs"] = IBI_sd_ISIs #len n_IBIs
    spike_stats_["IBI_dur"] = IBI_dur #len n_IBIs
    
    return spike_ids, isi_list, spikes_per_stim, av_isi_per_stim, sd_isi_per_stim, burst_ID
    #Len(n_spikes) = len(spike_ids) = len(burst_ID).
    #spike_ids: burst_ID: Burst ID assignment for each spike.

def write_worksheet(starts, stim_starts, ISIs, spike_id, threshold, info, is_burst): #Per input file writes an xlsx file containing list of ISI's, basic stats and filtered signal.
    #Writes a single worksheet.   
    #Workbook must already be open. 
    print("Writing Excel file.... ")
    '''if spike_id[0] == 0: #Need to fix this method, doesn't work if some stims don't have any spikes after them. Instead generate ID's by comparing each stim time to spike time.
        stim_sheet_indices = []
    else:
        stim_sheet_indices = [0]
    for i in range(len(spike_id)):
        try:
            if spike_id[i] != spike_id[i+1]:
                stim_sheet_indices.append(i)
        except:
            pass
    stim_sheet_indices = [i + 2 for i in stim_sheet_indices]'''
    if len(stim_starts) == 0:
        stim_starts.append(0)
    stim_sheet_indices = range(1,len(stim_starts)+1) #Lazy way for now, stim time column doesn't correspond to spike times.
    #print("stim sheet indices",stim_sheet_indices)
    
    worksheet = workbook.add_worksheet(trial_tags[trial_number].split(" ")[2] + " " + str(trial_number))
    col_title = workbook.add_format()
    col_title.set_text_wrap()
    col_title.set_bottom()
    col_title.set_bold()
    #col_title.set_italic()
    cell_right_border = workbook.add_format()
    cell_right_border.set_right()
    cell_title = workbook.add_format()
    cell_title.set_bold()
    col_title_border = workbook.add_format()
    col_title_border.set_right()
    col_title_border.set_bottom()
    col_title_border.set_text_wrap()
    col_title_border.set_bold()
    #stim_border = workbook.add_format()
    #col_title_border.set_italic()
    worksheet.write(0,0, "Stim time [s]", col_title)
    worksheet.write(0,1, "Spike time [s]", col_title)
    worksheet.write(0,2, "Stim Spike #", col_title)
    worksheet.write(0,3, "ISI [s]", col_title)
    worksheet.write(0,4, "Burst ID,", col_title_border)
    worksheet.write(0,6, "Burst #", col_title)
    worksheet.write(0,7, "n spikes/ burst", col_title)
    worksheet.write(0,8, "Burst durations [s]", col_title)
    worksheet.write(0,9, "Av. burst spike freq [Hz]", col_title)
    #worksheet.write(0,10, "σ burst spike freq [Hz]", col_title)
    worksheet.write(0,10, " ", col_title)
    worksheet.write(0,11, "IBI n spikes", col_title)
    worksheet.write(0,12, "IBI av ISI [s]", col_title)
    #worksheet.write(0,13, "IBI σ ISI [s]", col_title)
    worksheet.write(0,13, "IBI duration [s]", col_title_border)
    if len(stim_starts) > 1: #If it a stim trial...
        worksheet.write(0,16, "Stim #",col_title)
        worksheet.write(0,17, "n spikes/ stim", col_title)
        worksheet.write(0,18, "Av. ISI/ stim", col_title)
        worksheet.write(0,19, "σ ISI/ stim", col_title_border)
    else:
        worksheet.write(0,16, "Baseline Trial",col_title)
        worksheet.write(0,17, "n spikes", col_title)
        worksheet.write(0,18, "Av. ISI", col_title)
        worksheet.write(0,19, "σ ISI", col_title_border)

    worksheet.write(0,21, "Spike t bin (right edge)[s]", col_title)
    worksheet.write(0,22, "Bin Counts", col_title)
    worksheet.write(0,23, "Av. ISI/ bin", col_title_border)
    worksheet.write(0,25, "INFO:",col_title)
    worksheet.write(1,25, info)
    worksheet.write(3,25, "IBI = Interburst interval [s]")
    worksheet.write(4,25, "IBIs precede corresponding burst")
    worksheet.write(6,25, "BURST STATS:",cell_title)
    worksheet.write(9,25, "mean burst freq [Hz]",cell_title)
    worksheet.write(10,25, spike_stats_["mean_burst_freq"])
    worksheet.write(11,25, "σ burst spike freq [Hz]",cell_title)
    worksheet.write(12,25, spike_stats_["sd_burst_spike_freq"])
    worksheet.write(7,25, "n bursts",cell_title)
    worksheet.write(8,25, spike_stats_["n_bursts"])
    worksheet.write(13,25, "Av. IBI n spikes",cell_title)
    worksheet.write(14,25, spike_stats_["av_ibi_n_spikes"])
    worksheet.write(15,25, "Av. IBI duration [s]",cell_title)
    worksheet.write(16,25, spike_stats_["av_IBI_dur"])
    worksheet.write(17,25, "Av. IBI ISI [s]",cell_title)
    worksheet.write(18,25, spike_stats_["mean_IBI_ISI"])
    print("length stim sheet indices",len(stim_sheet_indices))
    print("length stim starts",len(stim_starts))
    print("Stim starts", stim_starts)
    for i in range(len(stim_starts)):
        worksheet.write(stim_sheet_indices[i],0, stim_starts[i], cell_right_border)
    [worksheet.write(i+1, 1 , starts[i]) for i in range(len(starts))]
    if max(spike_id) > 0:
        [worksheet.write(i+1, 2 , spike_id[i]) for i in range(len(spike_id))]
    [worksheet.write(i+1, 3 , ISIs[i]) for i in range(len(ISIs))]
    [worksheet.write(i+1, 4 , is_burst[i], cell_right_border) for i in range(len(is_burst))]
    [worksheet.write(i+1, 6 , i+1, cell_title) for i in range(spike_stats_["n_bursts"])]
    [worksheet.write(i+1, 7 , spike_stats_["spikes_per_burst"][i]) for i in range(len(spike_stats_["spikes_per_burst"]))] 
    [worksheet.write(i+1, 8 , spike_stats_["burst_durations"][i]) for i in range(len(spike_stats_["burst_durations"]))]
    [worksheet.write(i+1, 9 , spike_stats_["av_burst_spike_freq"][i]) for i in range(len(spike_stats_["av_burst_spike_freq"]))]
    #[worksheet.write(i+1, 10 , spike_stats_["sd_burst_spike_freq"][i]) for i in range(len(spike_stats_["sd_burst_spike_freq"]))]
    if spike_stats_["n_bursts"] > 0:
        spike_stats_["IBI_n_spikes"][0] = "-"  #For spreadsheet, replace first row of IBI stats with blank - these were not included in IBI stats calculations anyway...
        spike_stats_["IBI_av_ISIs"][0] = "-"
        spike_stats_["IBI_dur"][0] = "-"

    [worksheet.write(i+1, 11 , spike_stats_["IBI_n_spikes"][i]) for i in range(len(spike_stats_["IBI_n_spikes"]))]
    [worksheet.write(i+1, 12 , spike_stats_["IBI_av_ISIs"][i]) for i in range(len(spike_stats_["IBI_av_ISIs"]))]
    #[worksheet.write(i+1, 13 , spike_stats_["IBI_sd_ISIs"][i]) for i in range(len(spike_stats_["IBI_sd_ISIs"]))]
    [worksheet.write(j+1, 13 , spike_stats_["IBI_dur"][j], cell_right_border) for j in range(len(spike_stats_["IBI_dur"]))]
    if len(stim_starts) < 2: #then it baseline trial.
        #spikes_per_stim.pop(0)
        if type(spike_stats_["av_isi_per_stim"]) == list:
            worksheet.write(i+1, 18 , spike_stats_["av_isi_per_stim"][0]) 
            worksheet.write(i+1, 19 , spike_stats_["sd_isi_per_stim"][0], cell_right_border)
        else:
            worksheet.write(i+1, 18 , spike_stats_["av_isi_per_stim"]) 
            worksheet.write(i+1, 19 , spike_stats_["sd_isi_per_stim"], cell_right_border) 

    else: #stim trial
        if len(stim_starts) > 1:
            worksheet.write(1,16, "pre ---->")
            worksheet.write(len(spikes_per_stim),16, "post ---->")
        if type((spike_stats_["av_isi_per_stim"])) == list:
            [worksheet.write(i+1, 18 , spike_stats_["av_isi_per_stim"][i]) for i in range(len(spike_stats_["av_isi_per_stim"]))]            
        [worksheet.write(i+1, 19 , spike_stats_["sd_isi_per_stim"][i], cell_right_border) for i in range(len(spike_stats_["sd_isi_per_stim"]))]

    if len(spikes_per_stim) < 3: #baseline
        [worksheet.write(i+1, 17 , spikes_per_stim[0])] #spikes_per_stim is a global variable 
    else:
        [worksheet.write(i+1, 17 , spikes_per_stim[i]) for i in range(len(spikes_per_stim))] #spikes_per_stim is a global variable 
        [worksheet.write(i+1, 16 , i, cell_title) for i in range(len(spikes_per_stim))]
        if len(stim_starts) > 1:
            worksheet.write(1,16, "pre ---->")
            worksheet.write(len(spikes_per_stim),16, "post ---->")

    [worksheet.write(i+1, 21 , spike_stats_["start_t_bins"][i], cell_title) for i in range(1,len(spike_stats_["start_t_bins"]))]
    [worksheet.write(i+1, 22 , spike_stats_["starts_per_bin"][i]) for i in range(1,len(spike_stats_["starts_per_bin"]))]
    [worksheet.write(i+1, 23 , spike_stats_["isi_per_bin"][i], cell_right_border) for i in range(1,len(spike_stats_["isi_per_bin"]))]
    print("Trial " + str(trial_number) + " written")
    #End of write_worksheet function
'''======================================
=======     END OF FUNCTIONS     ========
======================================'''
#Start running program..............
Run = "yes"
#!!!!! Add info to graphs. Threshold, filter, y-axis values and x-axis.
#!!!! Graph title =  Trial number and threshold.
start_time = time.time()

''' #Old way of pre-stim-post assignment... delet?
trial_numbers = []
trial_tags = {} #Dictionary 
experiment_counter = 1
if divmod(stim_trial_numbers[0], 2)[1] == 0: # trials are even-numbered, must be channel 1 ;)
    print("Data being read on recording channel 1.")
else: # trials are odd-numbered, must be channel 2 ;)
    print("Data being read on recording channel 2.")
for x in stim_trial_numbers: #Generate experiment trial #'s for each stim #. Each stim # = 1 experiment.
    if n_channels > 1: #if 2, use odd or even steps for trial numbers, if 1 use integer steps.
        pre_trials = np.arange(x-n_channels*pre_n_trials, x, n_channels).tolist()
        post_trials = np.arange(x+n_channels, x+n_channels*(post_n_trials+1), n_channels).tolist()
    #elif n_channels == 1........ #!!!!!!!!!!!
    trial_numbers.append(pre_trials, [x], post_trials)
    for y in pre_trials:
        trial_tags[y] = "Ex" + experiment_counter + " pre "
    trial_tags[x] = "Ex" + experiment_counter + " STIM "
    for y in post_trials:
        trial_tags[y] = "Ex" + experiment_counter + " post "
    experiment_counter += 1
'''

#output list of trials, dictionary containing file tags with key = trial number: dict contents: Exp #, pre stim or post tag.

while Run == "yes":

    for spike_threshold in spike_thresholds: #Create unique Excel worksheet and graphs for each threshold, for each Experiment.

        for experiment_num in range(len(stim_trial_numbers)):
            #Declare Master (cumulative) variables for experiment: used for cumulative statistical analysis (and to be implemented: auto stim detection.)
            M_max_spike_per_burst = M_stim_starts = M_n_spikes = M_av_isi = M_sd_isi = M_n_bursts = M_av_burst_f = M_sd_burst_f = M_av_spike_per_burst = M_sd_spike_per_burst = M_max_spikes_burst = M_av_IBI_n_spikes = M_av_IBI_ISI = M_av_IBI_dur = []
            #trial_numbers = []
            trial_tags = {} #Dictionary 
            x = stim_trial_numbers[experiment_num]
            if n_channels > 1: #if 2, use odd or even steps for trial numbers, if 1 use integer steps.
                pre_trials = np.arange(x-n_channels*pre_n_trials, x, n_channels).tolist()
                post_trials = np.arange(x+n_channels, x+n_channels*(post_n_trials+1), n_channels).tolist()
            #elif n_channels == 1........
            # trial_numbers.append(pre_trials)
            # trial_numbers.append(x) 
            # trial_numbers.append(post_trials)
            
            # ====  Create new Excel file for each Eggsperiment =====.
            trial_numbers = pre_trials + [x] + post_trials
            bookname = file[:-4] + " Exp " + str(experiment_num+1) + " (Trials " + str(pre_trials[0]) + "-" + str(post_trials[-1]) + ") Thresh " + str(spike_threshold) + " data.xlsx"
            workbook = xlsxwriter.Workbook(bookname)
            summary_worksheet = workbook.add_worksheet("Summary")

            for y in pre_trials:
                trial_tags[y] = " Exp" + str(experiment_num+1) + " pre "
            trial_tags[x] = " Exp" + str(experiment_num+1) + " STIM "
            for y in post_trials:
                trial_tags[y] = " Exp" + str(experiment_num+1) + " post "
    
            for trial_number in trial_numbers:
                #One graph per trial. 
                #=======  Generate trial number  ============:
                print("Processing experiment number ",str(experiment_num+1), " trial number ",trial_number)
                trial_number = trial_number #It's true!
                
                #====== Pick trial to assign to master_data  =========           
                trial_data = data_float[(gap_list[trial_number]+1):(gap_list[trial_number+1])]
                master_data = np.array(trial_data)
                del trial_data
            
                #====== Trim data  =========           
                if t_analyze_window != "all": #Should the data be trimmed?
                    print("Data is being trimmed:")
                    if int(fs*t_analyze_window[0]) < len(master_data): 
                        t_startpoint = int(fs*t_analyze_window[0]) #[samples]
                        print("   t startpoint",t_startpoint, " samples")
                        if int(fs*t_analyze_window[1]) < len(master_data):
                            t_endpoint = int(fs*t_analyze_window[1]) #[samples]
                            print("   t_endpoint", t_endpoint, " samples")
                        else:
                            t_endpoint = int(len(master_data)) #[samples]
                            #print("endpoint not trimmed, t endpoint",t_endpoint, " samples")    
                    #trim from time 0 to start point
                    if t_startpoint > 0: #remove sample data from 0 to startpoint
                        #trimmed_data = np.delete(master_data, 1, range(0, t_startpoint+1)) #remove columns 
                        #trimmed_data = np.delete(master_data, 1, range(0, t_startpoint+1)) #remove columns 
                        trimmed_data = np.delete(master_data,np.s_[0:t_startpoint+1],axis=0)
                    else:
                        trimmed_data = master_data 
                    #trim from t_endpoint to end of data.
                    if len(master_data) > t_endpoint: #if data goes on beyond the specified end point time.
                        n_end_trim = len(master_data) - t_endpoint 
                        if n_end_trim > 0:
                            #print("end trim n",n_end_trim)
                            #trim from (last index - n_end_trim) to last index
                            #print("master data length",len(master_data))
                            #print("trimmed data shape",np.shape(trimmed_data))
                
                            n_row = int(np.shape(trimmed_data)[0])
                            #print("trimmed data type",type(trimmed_data))
                            #trimmed_data = np.delete(trimmed_data, 0, range(n_row - n_end_trim, n_row))
                            trimmed_data = np.delete(trimmed_data, np.s_[(n_row-n_end_trim): n_row],axis=0)
                    #print("original data shape",master_data.shape)
                    #print("trimmed_data shape",trimmed_data.shape)
                else: #no trimming occurs.
                    t_startpoint = 0 #[samples]
                    #print("master_Data type",type(master_data))
                    if type(master_data) == list:
                        t_endpoint = int(len(master_data)) #[samples]
                    else:
                        t_endpoint = int(len(master_data)) #[samples]
                    #print("data not trimmed. t startpoint",t_startpoint)
                    #print("   t endpoint",t_endpoint)
                    trimmed_data = master_data 
                end_time = time.time()
                time_elapsed = end_time - start_time
                #print("time elapsed trimming data",time_elapsed)
                #print("t_endpoint [samples]: ",t_endpoint)
                y_values = trimmed_data
                del trimmed_data
                
                #==========  STIM AND EPSP PROCESSING  ===========
                raw_unblanked_sig = np.array(y_values)
                
                stim_starts, stim_stops, stim_widths, stim_n_spikes = stim_train_detect(y_values,stim_peak) # Generate stim_starts etc.
                if not trial_number in stim_trial_numbers: #if it not stim trial , we don't care about stims
                    stim_starts = [0]
                    stim_stops = [0]
                    stim_widths = [0]
                    stim_n_spikes = 1
                y_values, stim_blank_starts, stim_blank_stops, stim_midpoints = remove_stim_artefacts(stim_starts, stim_stops, y_values) # Remove stim artefact from signal
                y_values, epsp_starts, epsp_stops = remove_epsps(stim_starts, stim_stops, y_values) #remove EPSP from signal
                epsp_starts = [0]
                epsp_stops = [0]
                #  ========  UNIT SCALING, SAMPLES -> SECONDS: ================================= 
                if t_analyze_window == "all":
                    trim_offset = 0
                else:
                    trim_offset = float(t_analyze_window[0])
        
                stim_starts_secs = [x/fs for x in stim_starts]
                stim_starts_scaled = [x+trim_offset for x in stim_starts_secs]
                stim_stops_secs = [x/fs for x in stim_stops]
                stim_stops_scaled = [x+trim_offset for x in stim_stops_secs]
                epsp_starts_scaled = [sample_to_sec(x) for x in epsp_starts]
                epsp_stops_scaled = [sample_to_sec(x) for x in epsp_stops]
                
                #raw_blanked_sig = np.array(y_values)
                
                # =========  APPLY FILTER   ============
                if filter_type == "bandpass":
                    #print("band pass filter ",lowcut, highcut, "Hz")
                    filtered_signal = butter_bandpass_filter(y_values, lowcut, highcut, fs, order=filter_order) # filter is on
                elif filter_type == "highpass":
                    #print("high pass filter ",lowcut, "Hz")
                    filtered_signal = high_pass_filter(y_values, lowcut, fs, order=filter_order)
                elif filter_type == "lowpass":
                    #print("low pass filter ",lowcut, "Hz")
                    filtered_signal = low_pass_filter2(y_values, highcut, fs, order=filter_order)
                elif filter_type == "none":
                    #print("No filter applied")
                    filtered_signal = y_values
                else:
                    print("Filter type not entered correctly. Defaulting to #nofilter.")
                    filtered_signal = y_values
                
                #Write audio 
                '''title = file[:-4] + " Trial " + str(trial_number) + " filtered"
                write(title + ".wav", fs, filtered_signal) #write data to a wav file'''
        
                #Collect dot positions per threshold. 
                spike_threshold = spike_threshold #you know it true!
                #  ========  SPIKE DETECTION & STATS: ===================================
                starts, stops, widths, n_spikes = generate_starts_stops(filtered_signal,spike_threshold)
                spike_IDs, ISIs, spikes_per_stim, av_ISI_per_stim, sd_ISI_per_stim, bursts = spike_stats(stim_starts, starts)
                #Test of dictionary:
    
                #print("spike_IDs",spike_IDs)
                #print("ISIs",ISIs)
                #  ========  UNIT SCALING, SAMPLES -> SECONDS: ================================= 
                starts_secs = [x/fs for x in starts]
                starts_scaled = [x+trim_offset for x in starts_secs]
                # ==========  WRITE EXCEL          ==========================================
                info = "Threshold: "+str(spike_threshold)+"mV; filter:"+filter_type+ " "+ str(lowcut) + ", " + str(highcut)+ "Hz; Trial length : "+ str(sample_to_sec(len(y_values))) + "s; EPSP Highcut: " + str(epsp_highcut) + "Hz"
                #info2 is stim artefact detection stuff.
                info2 = "; Stim blanking: "+str(stim_blanking)+ "ms; Stim peak: " + str(stim_peak) + "mV; Slope dt: "+ str(slope_dt) + "samples; Stim slope thresh: " + str(stim_slope_thresh) + "mV; stim refractory: " + str(stim_refractory) + "ms; Max spike freq:" + str(max_spike_freq) + "Hz; Min. burst freq" + str(min_burst_freq) +"Hz"
                info = info + info2
                write_worksheet(starts_scaled, stim_starts_scaled, ISIs, spike_IDs, spike_threshold, info, bursts)
                
                # ==========  WRITE TO CUMULATIVE STATS ARRAY  for each trial ==========================================
                '''M_n_spikes.append(n_spikes)
                M_av_isi.append(stats.mean(ISIs)) #!!!!!!!!!!! Check units of ISIs are seconds?
                M_sd_isi.append(stats.stdev(ISIs))
                M_n_bursts.append(spike_stats_["n_bursts"])
                M_av_burst_f.append(spike_stats_["mean_burst_freq"]) 
                M_sd_burst_f.append(spike_stats_["sd_burst_spike_freq"])
                M_av_spike_per_burst.append(stats.mean(spike_stats_["spikes_per_burst"]))
                M_sd_spike_per_burst.append(stats.stdev(spike_stats_["spikes_per_burst"]))
                M_max_spike_per_burst.append(max(spike_stats_["spikes_per_burst"]))
                M_av_IBI_n_spikes.append(spike_stats_["av_ibi_n_spikes"])
                M_av_IBI_ISI.append(spike_stats_["mean_IBI_ISI"])
                M_av_IBI_dur.append(spike_stats_["av_IBI_dur"])'''
                # ==========  GRAPHING          ==========================================
                
                #Create t-axis values 
                t_values = np.arange(t_startpoint, t_endpoint-1) #x axis units are samples -_____-
                if t_display_units == "milliseconds":
                    t_values_graph = (t_values/fs)*1000
                elif t_display_units == "seconds":
                    t_values_graph = (t_values/fs)
                else:
                    print("Invalid entry for t_display_units. Defaulting to units of seconds.")
                    t_values_graph = (t_values/fs)
                
                #==========   FIRST GRAPH - FILTERED SIG   ===============
                plt.style.use('seaborn-whitegrid')
                plt.figure(figsize=graph_size)
                title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number)  + " thresh" + str(spike_threshold) + " filt.png"
                plt.title(title)
                plt.xlabel(t_display_units)
                    #Plot neural data
                try:
                    plt.plot(t_values_graph, filtered_signal, linewidth=1) # # filtered signal
                    #plt.plot(t_values_graph, raw_unblanked_sig, linewidth=0.5, c="Red") # raw signal
                    #plt.plot(t_values_graph, filtered_signal, 'go')
                except:
                    plt.plot(t_values_graph, filtered_signal[:-1], linewidth=1) # filtered signal
                    #plt.plot(t_values_graph, raw_unblanked_sig[:-1], linewidth=0.5, c="Red") # raw signal
                    #plt.plot(t_values_graph, filtered_signal[:-1], 'go')
                plt.scatter(epsp_starts_scaled, np.full(len(epsp_starts_scaled), 0.02), marker="x", s=120, c="Yellow") #EPSP starts
                plt.scatter(epsp_stops_scaled, np.full(len(epsp_stops_scaled), 0.02), marker="x", s=120, c="Green") #EPSP stops
                plt.scatter(starts_scaled, np.full(len(starts), spike_threshold), marker="D", c="Red") #Plot spike dots
                if stim_n_spikes > 0:
                    plt.scatter(stim_starts_scaled, np.full(stim_n_spikes, 0.1), marker="D", c="Orange") #Stim starts
                plt.scatter(stim_stops_scaled, np.full(len(stim_stops_scaled), 0.1), marker="x", c="Orange") #Stim stops
                #plt.yticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_x_interval, graph_x_interval))
                if graph_x_interval != "auto":
                    plt.xticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_y_interval, graph_y_interval))
                for xc in stim_starts_scaled:
                    plt.axvline(x=xc, c="Yellow")
                plt.tight_layout()
                #plt.show
                plt.savefig(title)
                plt.clf()
                plt.close('all')
                
                #==========   SECOND GRAPH - ORIGINAL SIG   ===============
                plt.figure(figsize=graph_size)
                title = file[:-4] + trial_tags[trial_number] + " Trial " + str(trial_number) + " thresh" + str(spike_threshold) + " orig+filt.png"
                plt.title(title)
                plt.xlabel(t_display_units)
                    #Plot neural data
                try:
                    plt.plot(t_values_graph, filtered_signal, linewidth=1) # # filtered signal
                    plt.plot(t_values_graph, raw_unblanked_sig, linewidth=0.5, c="Red") # raw signal
                    #plt.plot(t_values_graph, filtered_signal, 'go')
                except:
                    plt.plot(t_values_graph, filtered_signal[:-1], linewidth=1) # filtered signal
                    plt.plot(t_values_graph, raw_unblanked_sig[:-1], linewidth=0.5, c="Red") # raw signal
                    #plt.plot(t_values_graph, filtered_signal[:-1], 'go')
                plt.scatter(epsp_starts_scaled, np.full(len(epsp_starts_scaled), 0.02), marker="x", s=120, c="Yellow") #EPSP starts
                plt.scatter(epsp_stops_scaled, np.full(len(epsp_stops_scaled), 0.02), marker="x", s=120, c="Green") #EPSP stops
                plt.scatter(starts_scaled, np.full(len(starts), spike_threshold), marker="D", c="Red") #Plot spike dots
                if stim_n_spikes > 0:
                    plt.scatter(stim_starts_scaled, np.full(stim_n_spikes, 0.1), marker="D", c="Orange") #Stim starts
                plt.scatter(stim_stops_scaled, np.full(len(stim_stops_scaled), 0.1), marker="x", c="Orange") #Stim stops
                if graph_x_interval != "auto":
                    plt.xticks(np.arange(min(t_values_graph), max(t_values_graph)+graph_x_interval, graph_x_interval))
                for xc in stim_starts_scaled:
                    plt.axvline(x=xc, c="Yellow")
                plt.tight_layout()
                #plt.show
                plt.savefig(title)
                plt.clf()
                plt.close('all')
            #Summary stats at this level of indent ;)
            #Slice Master variables according to pre, stim, post indices.
            '''
            pre_M_n_spikes = M_n_spikes[pre_trials[0]:pre_trials[-1]]
            pre_M_av_isi = M_av_isi[pre_trials[0]:pre_trials[-1]]
            pre_M_sd_isi = M_sd_isi[pre_trials[0]:pre_trials[-1]]
            pre_M_n_bursts = M_n_bursts[pre_trials[0]:pre_trials[-1]]
            pre_M_av_burst_f = M_av_burst_f[pre_trials[0]:pre_trials[-1]]
            pre_M_sd_burst_f = M_sd_burst_f[pre_trials[0]:pre_trials[-1]]
            pre_M_av_spike_per_burst = M_av_spike_per_burst[pre_trials[0]:pre_trials[-1]]
            pre_M_sd_spike_per_burst = M_sd_spike_per_burst[pre_trials[0]:pre_trials[-1]]
            pre_M_max_spike_per_burst = M_max_spike_per_burst[pre_trials[0]:pre_trials[-1]]
            pre_M_av_IBI_n_spikes = M_av_IBI_n_spikes[pre_trials[0]:pre_trials[-1]]
            pre_M_av_IBI_ISI = M_av_IBI_ISI[pre_trials[0]:pre_trials[-1]]
            pre_M_av_IBI_dur = M_av_IBI_dur[pre_trials[0]:pre_trials[-1]]

            stim_M_n_spikes = M_n_spikes[max(pre_trials)+1]
            stim_M_av_isi = M_av_isi[max(pre_trials)+1]
            stim_M_sd_isi = M_sd_isi[max(pre_trials)+1]
            stim_M_n_bursts = M_n_bursts[max(pre_trials)+1]
            stim_M_av_burst_f = M_av_burst_f[max(pre_trials)+1]
            stim_M_sd_burst_f = M_sd_burst_f[max(pre_trials)+1]
            stim_M_av_spike_per_burst = M_av_spike_per_burst[max(pre_trials)+1]
            stim_M_sd_spike_per_burst = M_sd_spike_per_burst[max(pre_trials)+1]
            stim_M_max_spike_per_burst = M_max_spike_per_burst[max(pre_trials)+1]
            stim_M_av_IBI_n_spikes = M_av_IBI_n_spikes[max(pre_trials)+1]
            stim_M_av_IBI_ISI = M_av_IBI_ISI[max(pre_trials)+1]
            stim_M_av_IBI_dur = M_av_IBI_dur[max(pre_trials)+1]'''
            #!!!!!!!!!!! left off here
            workbook.close()
            gc.collect()

        # Write filtered signal to a worksheet =============================:
        '''print("Writing filtered signal data to Excel spreadsheet...")
        worksheet = workbook.add_worksheet("Filtered signal")
        [worksheet.write(i, 0, filtered_signal[i]) for i in range(len(filtered_signal))]
        worksheet.write(0, 1, "sig blank starts")
        worksheet.write(0, 2, "sig blank stops")
        [worksheet.write(i+1, 1, stim_blank_starts[i]) for i in range(len(stim_blank_starts))]
        [worksheet.write(i+1, 2, stim_blank_stops[i]) for i in range(len(stim_blank_stops))]'''
    
        #title_number += 1
    # FINISHED RUNNING EXPERIMENT ===================================================:
   
        
    end_time = time.time()
    time_elapsed = end_time - master_start_time
    print("Program time elapsed:",time_elapsed," seconds.")
    print("Current time:",time.localtime())
    
    #os.system('say "program finished"')
    print("Program finished running without any problems ¯\_( ͡❛ ͜ʖ ͡❛)_/¯")
    
    #Ask if user wants to change any parameters and run again?
    user_halt = True
    while user_halt == True:
        choice = "Stop"
        #choice = input("Enter 'Run' to run again with new parameters or 'Stop' to exit program \n")
        if choice == "Run":
            while user_halt == True:
                param = input("Enter a variable name from User Parameters that you want to change, or 'Run' to run. \n")
                print(param)
                if param != "Run":
                    param_value = input(('Enter new value for ',param))
                    if param != "filter_type" and param != "t_analyze_window":
                        vars()[param] = float(param_value)
                    elif param == "graph_x_interval":
                        if param_value == "auto":
                            graph_x_interval = "auto"
                        else:
                            vars()[graph_x_interval] = param_value
                    else:
                        vars()[param] = param_value
                    print(param, "now = ", param_value)
                else:
                    print("Programming running again....")
                    user_halt = False
                    break
                    break
                    break
        elif choice == "Stop":
            print("Program terminating. 𝔾𝕠𝕠𝕕𝕓𝕪𝕖.")
            Run = "No"
            del data_float
            del y_values
            del t_values
            del t_values_graph
            del master_data
            del raw_unblanked_sig
            break
        else:
            print("You did not enter a correct input :-/ , please try again...")
